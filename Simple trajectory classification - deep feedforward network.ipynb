{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method and Result \n",
    "\n",
    "Using an appropriately tuned deep feedforward neural network to train the data ($2000$ training examples, with each example consisting of a time series/trajectory for the $x$-coordinate of 102 steps), we found that accuracy on the test data ($1000$ test examples, with each example consisting of a trajectory for the $x$-coordinate of 102 steps) is around $93 \\%$ (a significant improvement over the accuracy of a random baseline). The accuracy rate could be further improved by using, for instance, more training data, including the trajectory for the $y$-coordinate in the training data set, or a more sophisticated neural network architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('fivethirtyeight')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "# First, we get the data\n",
    "dataset0 = pd.read_csv(\"/Users/soonhoe/Desktop/AAA Python/trajx_m0.csv\",header=None)\n",
    "dataset1 = pd.read_csv(\"/Users/soonhoe/Desktop/AAA Python/trajx_m1.csv\",header=None)\n",
    "dataset2 = pd.read_csv(\"/Users/soonhoe/Desktop/AAA Python/trajx_m2.csv\",header=None)\n",
    "\n",
    "li = []\n",
    "li.append(dataset0)\n",
    "li.append(dataset1)\n",
    "li.append(dataset2)\n",
    "\n",
    "dataset = np.array(pd.concat(li, axis=0, ignore_index=True))\n",
    "np.random.shuffle(dataset)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "np.savetxt('all_traj.csv', np.c_[dataset], delimiter=',') \n",
    "\n",
    "dataset.head(10)\n",
    "row=np.shape(dataset)[0]\n",
    "col=np.shape(dataset)[1]\n",
    "print(row); print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 102)\n",
      "(1000, 102)\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "num_train = int(row*2/3)\n",
    "training_set = dataset[:num_train].iloc[:,0:col].values\n",
    "test_set = dataset[num_train:].iloc[:,0:col].values\n",
    "print(np.shape(training_set)); print(np.shape(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>-0.014919</td>\n",
       "      <td>-0.025271</td>\n",
       "      <td>-0.054580</td>\n",
       "      <td>-0.047405</td>\n",
       "      <td>-0.036849</td>\n",
       "      <td>-0.045826</td>\n",
       "      <td>-0.070926</td>\n",
       "      <td>-0.058524</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.456992</td>\n",
       "      <td>-0.455515</td>\n",
       "      <td>-0.475321</td>\n",
       "      <td>-0.495757</td>\n",
       "      <td>-0.470785</td>\n",
       "      <td>-0.501330</td>\n",
       "      <td>-0.519588</td>\n",
       "      <td>-0.511101</td>\n",
       "      <td>-0.523270</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010122</td>\n",
       "      <td>0.026897</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.026228</td>\n",
       "      <td>0.031618</td>\n",
       "      <td>0.032011</td>\n",
       "      <td>-0.009722</td>\n",
       "      <td>-0.016024</td>\n",
       "      <td>-0.027293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136431</td>\n",
       "      <td>0.131556</td>\n",
       "      <td>0.120954</td>\n",
       "      <td>0.121202</td>\n",
       "      <td>0.120716</td>\n",
       "      <td>0.108841</td>\n",
       "      <td>0.097936</td>\n",
       "      <td>0.110991</td>\n",
       "      <td>0.115918</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.014335</td>\n",
       "      <td>-0.004937</td>\n",
       "      <td>0.012488</td>\n",
       "      <td>0.021524</td>\n",
       "      <td>0.043106</td>\n",
       "      <td>0.027564</td>\n",
       "      <td>0.026114</td>\n",
       "      <td>0.032234</td>\n",
       "      <td>0.011147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103494</td>\n",
       "      <td>0.077861</td>\n",
       "      <td>0.051864</td>\n",
       "      <td>0.039734</td>\n",
       "      <td>0.044114</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>-0.007039</td>\n",
       "      <td>-0.028365</td>\n",
       "      <td>-0.057913</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.012685</td>\n",
       "      <td>-0.021520</td>\n",
       "      <td>-0.029870</td>\n",
       "      <td>-0.017310</td>\n",
       "      <td>-0.023700</td>\n",
       "      <td>-0.027145</td>\n",
       "      <td>-0.042304</td>\n",
       "      <td>-0.056873</td>\n",
       "      <td>-0.049443</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056277</td>\n",
       "      <td>-0.047090</td>\n",
       "      <td>-0.030296</td>\n",
       "      <td>-0.038074</td>\n",
       "      <td>-0.043922</td>\n",
       "      <td>-0.051046</td>\n",
       "      <td>-0.054153</td>\n",
       "      <td>-0.028927</td>\n",
       "      <td>-0.015559</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.010167</td>\n",
       "      <td>-0.012033</td>\n",
       "      <td>-0.007883</td>\n",
       "      <td>-0.006042</td>\n",
       "      <td>-0.025921</td>\n",
       "      <td>-0.025726</td>\n",
       "      <td>-0.034747</td>\n",
       "      <td>-0.028961</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236607</td>\n",
       "      <td>-0.245837</td>\n",
       "      <td>-0.231785</td>\n",
       "      <td>-0.234614</td>\n",
       "      <td>-0.229503</td>\n",
       "      <td>-0.227589</td>\n",
       "      <td>-0.230369</td>\n",
       "      <td>-0.239519</td>\n",
       "      <td>-0.215754</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012921</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>0.036221</td>\n",
       "      <td>0.034004</td>\n",
       "      <td>0.014015</td>\n",
       "      <td>0.005883</td>\n",
       "      <td>0.006194</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>-0.012036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.451704</td>\n",
       "      <td>-0.450624</td>\n",
       "      <td>-0.450703</td>\n",
       "      <td>-0.464482</td>\n",
       "      <td>-0.460956</td>\n",
       "      <td>-0.481136</td>\n",
       "      <td>-0.486432</td>\n",
       "      <td>-0.504481</td>\n",
       "      <td>-0.486251</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.003537</td>\n",
       "      <td>0.023761</td>\n",
       "      <td>0.029217</td>\n",
       "      <td>0.046976</td>\n",
       "      <td>0.038772</td>\n",
       "      <td>0.032941</td>\n",
       "      <td>0.011174</td>\n",
       "      <td>-0.005505</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027107</td>\n",
       "      <td>-0.011182</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.007328</td>\n",
       "      <td>0.020003</td>\n",
       "      <td>0.034761</td>\n",
       "      <td>0.026486</td>\n",
       "      <td>0.013341</td>\n",
       "      <td>-0.005342</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012640</td>\n",
       "      <td>0.038045</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>0.007516</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>-0.005328</td>\n",
       "      <td>0.002933</td>\n",
       "      <td>0.009642</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919888</td>\n",
       "      <td>0.927188</td>\n",
       "      <td>0.958612</td>\n",
       "      <td>0.947647</td>\n",
       "      <td>0.996720</td>\n",
       "      <td>1.023411</td>\n",
       "      <td>1.033983</td>\n",
       "      <td>1.049808</td>\n",
       "      <td>1.053242</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001153</td>\n",
       "      <td>-0.013394</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.012625</td>\n",
       "      <td>0.018496</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.024891</td>\n",
       "      <td>0.029608</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207476</td>\n",
       "      <td>0.202096</td>\n",
       "      <td>0.205030</td>\n",
       "      <td>0.203118</td>\n",
       "      <td>0.216371</td>\n",
       "      <td>0.236962</td>\n",
       "      <td>0.219202</td>\n",
       "      <td>0.215428</td>\n",
       "      <td>0.233862</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.020120</td>\n",
       "      <td>-0.007772</td>\n",
       "      <td>-0.014959</td>\n",
       "      <td>-0.031024</td>\n",
       "      <td>-0.024136</td>\n",
       "      <td>-0.005736</td>\n",
       "      <td>-0.035497</td>\n",
       "      <td>-0.048936</td>\n",
       "      <td>-0.058170</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.474206</td>\n",
       "      <td>-0.490314</td>\n",
       "      <td>-0.512202</td>\n",
       "      <td>-0.506013</td>\n",
       "      <td>-0.503897</td>\n",
       "      <td>-0.526692</td>\n",
       "      <td>-0.540602</td>\n",
       "      <td>-0.555435</td>\n",
       "      <td>-0.556029</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3         4         5         6         7    \\\n",
       "0  0.0  0.005917 -0.014919 -0.025271 -0.054580 -0.047405 -0.036849 -0.045826   \n",
       "1  0.0  0.010122  0.026897  0.001341  0.026228  0.031618  0.032011 -0.009722   \n",
       "2  0.0 -0.014335 -0.004937  0.012488  0.021524  0.043106  0.027564  0.026114   \n",
       "3  0.0 -0.012685 -0.021520 -0.029870 -0.017310 -0.023700 -0.027145 -0.042304   \n",
       "4  0.0  0.002505  0.010167 -0.012033 -0.007883 -0.006042 -0.025921 -0.025726   \n",
       "5  0.0  0.012921  0.019048  0.036221  0.034004  0.014015  0.005883  0.006194   \n",
       "6  0.0  0.000535  0.003537  0.023761  0.029217  0.046976  0.038772  0.032941   \n",
       "7  0.0  0.012640  0.038045  0.009576  0.007516 -0.000270 -0.005328  0.002933   \n",
       "8  0.0 -0.001153 -0.013394  0.006030  0.012625  0.018496  0.011931  0.024891   \n",
       "9  0.0 -0.020120 -0.007772 -0.014959 -0.031024 -0.024136 -0.005736 -0.035497   \n",
       "\n",
       "        8         9    ...       92        93        94        95        96   \\\n",
       "0 -0.070926 -0.058524  ... -0.456992 -0.455515 -0.475321 -0.495757 -0.470785   \n",
       "1 -0.016024 -0.027293  ...  0.136431  0.131556  0.120954  0.121202  0.120716   \n",
       "2  0.032234  0.011147  ...  0.103494  0.077861  0.051864  0.039734  0.044114   \n",
       "3 -0.056873 -0.049443  ... -0.056277 -0.047090 -0.030296 -0.038074 -0.043922   \n",
       "4 -0.034747 -0.028961  ... -0.236607 -0.245837 -0.231785 -0.234614 -0.229503   \n",
       "5  0.001293 -0.012036  ... -0.451704 -0.450624 -0.450703 -0.464482 -0.460956   \n",
       "6  0.011174 -0.005505  ... -0.027107 -0.011182  0.002101  0.007328  0.020003   \n",
       "7  0.009642  0.000144  ...  0.919888  0.927188  0.958612  0.947647  0.996720   \n",
       "8  0.029608  0.052734  ...  0.207476  0.202096  0.205030  0.203118  0.216371   \n",
       "9 -0.048936 -0.058170  ... -0.474206 -0.490314 -0.512202 -0.506013 -0.503897   \n",
       "\n",
       "        97        98        99        100  101  \n",
       "0 -0.501330 -0.519588 -0.511101 -0.523270  1.0  \n",
       "1  0.108841  0.097936  0.110991  0.115918  0.0  \n",
       "2  0.020761 -0.007039 -0.028365 -0.057913  2.0  \n",
       "3 -0.051046 -0.054153 -0.028927 -0.015559  0.0  \n",
       "4 -0.227589 -0.230369 -0.239519 -0.215754  0.0  \n",
       "5 -0.481136 -0.486432 -0.504481 -0.486251  1.0  \n",
       "6  0.034761  0.026486  0.013341 -0.005342  0.0  \n",
       "7  1.023411  1.033983  1.049808  1.053242  2.0  \n",
       "8  0.236962  0.219202  0.215428  0.233862  0.0  \n",
       "9 -0.526692 -0.540602 -0.555435 -0.556029  1.0  \n",
       "\n",
       "[10 rows x 102 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = pd.DataFrame(training_set)\n",
    "training_df.head(10)\n",
    "\n",
    "#pd.plotting.scatter_matrix(training_df, c=training_set['750'], figsize=(15,15), marker='o', s=60)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 train samples\n",
      "1000 test samples\n",
      "(2000, 101)\n",
      "(1000, 101)\n",
      "(2000, 1)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "#from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "#hyperparameters\n",
    "batch_size = 500\n",
    "epochs = 800\n",
    "\n",
    "#number of categories\n",
    "num_classes = 3\n",
    "\n",
    "# the data, split between train and test sets\n",
    "x_train=training_set[:,:col-1]\n",
    "y_train=training_set[:,col-1:]\n",
    "x_test=test_set[:,:col-1]\n",
    "y_test=test_set[:,col-1:]\n",
    "\n",
    "x_train = x_train.reshape(num_train, col-1)\n",
    "x_test = x_test.reshape(row-num_train, col-1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "#x_train /= 255\n",
    "#x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print(np.shape(x_train)); print(np.shape(x_test))\n",
    "print(np.shape(y_train)); print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.329"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy of a random baseline\n",
    "import copy\n",
    "y_test_copy=copy.copy(y_test)\n",
    "np.random.shuffle(y_test_copy)\n",
    "float(np.sum(np.array(y_test)==np.array(y_test_copy)))/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               13056     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 29,955\n",
      "Trainable params: 29,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/800\n",
      "2000/2000 [==============================] - 2s 759us/step - loss: 1.1532 - acc: 0.3405 - val_loss: 1.0464 - val_acc: 0.6500\n",
      "Epoch 2/800\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 1.0914 - acc: 0.3885 - val_loss: 1.0122 - val_acc: 0.5180\n",
      "Epoch 3/800\n",
      "2000/2000 [==============================] - 0s 68us/step - loss: 1.0611 - acc: 0.4320 - val_loss: 1.0020 - val_acc: 0.3400\n",
      "Epoch 4/800\n",
      "2000/2000 [==============================] - 0s 58us/step - loss: 1.0261 - acc: 0.4445 - val_loss: 0.9590 - val_acc: 0.6550\n",
      "Epoch 5/800\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 1.0027 - acc: 0.489 - 0s 67us/step - loss: 0.9990 - acc: 0.4850 - val_loss: 0.9444 - val_acc: 0.6230\n",
      "Epoch 6/800\n",
      "2000/2000 [==============================] - 0s 72us/step - loss: 0.9702 - acc: 0.5220 - val_loss: 0.9160 - val_acc: 0.6620\n",
      "Epoch 7/800\n",
      "2000/2000 [==============================] - 0s 43us/step - loss: 0.9493 - acc: 0.5255 - val_loss: 0.9039 - val_acc: 0.5360\n",
      "Epoch 8/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.9355 - acc: 0.5325 - val_loss: 0.8851 - val_acc: 0.6500\n",
      "Epoch 9/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.8923 - acc: 0.5535 - val_loss: 0.8818 - val_acc: 0.6400\n",
      "Epoch 10/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.8914 - acc: 0.5715 - val_loss: 0.8567 - val_acc: 0.6480\n",
      "Epoch 11/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.8722 - acc: 0.5790 - val_loss: 0.8438 - val_acc: 0.6460\n",
      "Epoch 12/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.8529 - acc: 0.5930 - val_loss: 0.8292 - val_acc: 0.6450\n",
      "Epoch 13/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.8546 - acc: 0.5825 - val_loss: 0.8148 - val_acc: 0.6460\n",
      "Epoch 14/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.8301 - acc: 0.5845 - val_loss: 0.7976 - val_acc: 0.6620\n",
      "Epoch 15/800\n",
      "2000/2000 [==============================] - 0s 50us/step - loss: 0.8046 - acc: 0.6015 - val_loss: 0.7826 - val_acc: 0.6570\n",
      "Epoch 16/800\n",
      "2000/2000 [==============================] - 0s 45us/step - loss: 0.8092 - acc: 0.6015 - val_loss: 0.7717 - val_acc: 0.6540\n",
      "Epoch 17/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.7886 - acc: 0.6060 - val_loss: 0.7521 - val_acc: 0.6520\n",
      "Epoch 18/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.7678 - acc: 0.6345 - val_loss: 0.7352 - val_acc: 0.6580\n",
      "Epoch 19/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.7570 - acc: 0.6310 - val_loss: 0.7256 - val_acc: 0.7210\n",
      "Epoch 20/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.7359 - acc: 0.6590 - val_loss: 0.7064 - val_acc: 0.6610\n",
      "Epoch 21/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.7196 - acc: 0.6500 - val_loss: 0.6905 - val_acc: 0.6840\n",
      "Epoch 22/800\n",
      "2000/2000 [==============================] - 0s 46us/step - loss: 0.7035 - acc: 0.6680 - val_loss: 0.6760 - val_acc: 0.6990\n",
      "Epoch 23/800\n",
      "2000/2000 [==============================] - 0s 43us/step - loss: 0.7118 - acc: 0.6390 - val_loss: 0.6726 - val_acc: 0.6560\n",
      "Epoch 24/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.6877 - acc: 0.6755 - val_loss: 0.6499 - val_acc: 0.6930\n",
      "Epoch 25/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.6780 - acc: 0.6870 - val_loss: 0.6418 - val_acc: 0.6790\n",
      "Epoch 26/800\n",
      "2000/2000 [==============================] - 0s 42us/step - loss: 0.6772 - acc: 0.6760 - val_loss: 0.6319 - val_acc: 0.7530\n",
      "Epoch 27/800\n",
      "2000/2000 [==============================] - 0s 48us/step - loss: 0.6705 - acc: 0.6875 - val_loss: 0.6190 - val_acc: 0.7420\n",
      "Epoch 28/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.6561 - acc: 0.6880 - val_loss: 0.6094 - val_acc: 0.7200\n",
      "Epoch 29/800\n",
      "2000/2000 [==============================] - 0s 44us/step - loss: 0.6584 - acc: 0.6925 - val_loss: 0.5984 - val_acc: 0.7460\n",
      "Epoch 30/800\n",
      "2000/2000 [==============================] - 0s 66us/step - loss: 0.6502 - acc: 0.7035 - val_loss: 0.5956 - val_acc: 0.7060\n",
      "Epoch 31/800\n",
      "2000/2000 [==============================] - 0s 70us/step - loss: 0.6363 - acc: 0.7075 - val_loss: 0.5892 - val_acc: 0.7110\n",
      "Epoch 32/800\n",
      "2000/2000 [==============================] - 0s 79us/step - loss: 0.6273 - acc: 0.7180 - val_loss: 0.5744 - val_acc: 0.7560\n",
      "Epoch 33/800\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 0.6099 - acc: 0.7270 - val_loss: 0.5666 - val_acc: 0.7720\n",
      "Epoch 34/800\n",
      "2000/2000 [==============================] - 0s 73us/step - loss: 0.6022 - acc: 0.7415 - val_loss: 0.5603 - val_acc: 0.7830\n",
      "Epoch 35/800\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 0.6005 - acc: 0.7265 - val_loss: 0.5519 - val_acc: 0.7970\n",
      "Epoch 36/800\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 0.5841 - acc: 0.7495 - val_loss: 0.5472 - val_acc: 0.8000\n",
      "Epoch 37/800\n",
      "2000/2000 [==============================] - 0s 76us/step - loss: 0.5997 - acc: 0.7355 - val_loss: 0.5397 - val_acc: 0.8100\n",
      "Epoch 38/800\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 0.5915 - acc: 0.7445 - val_loss: 0.5364 - val_acc: 0.8200\n",
      "Epoch 39/800\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 0.5797 - acc: 0.7570 - val_loss: 0.5308 - val_acc: 0.8150\n",
      "Epoch 40/800\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 0.5763 - acc: 0.7510 - val_loss: 0.5242 - val_acc: 0.8410\n",
      "Epoch 41/800\n",
      "2000/2000 [==============================] - 0s 45us/step - loss: 0.5688 - acc: 0.7570 - val_loss: 0.5170 - val_acc: 0.8350\n",
      "Epoch 42/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.5712 - acc: 0.7600 - val_loss: 0.5138 - val_acc: 0.8300\n",
      "Epoch 43/800\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 0.5588 - acc: 0.7650 - val_loss: 0.5084 - val_acc: 0.8260\n",
      "Epoch 44/800\n",
      "2000/2000 [==============================] - 0s 75us/step - loss: 0.5588 - acc: 0.7705 - val_loss: 0.5090 - val_acc: 0.8610\n",
      "Epoch 45/800\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 0.5475 - acc: 0.7695 - val_loss: 0.5043 - val_acc: 0.8230\n",
      "Epoch 46/800\n",
      "2000/2000 [==============================] - 0s 44us/step - loss: 0.5556 - acc: 0.7685 - val_loss: 0.4981 - val_acc: 0.8500\n",
      "Epoch 47/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.5555 - acc: 0.7615 - val_loss: 0.4936 - val_acc: 0.8500\n",
      "Epoch 48/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.5432 - acc: 0.7825 - val_loss: 0.4899 - val_acc: 0.8500\n",
      "Epoch 49/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.5423 - acc: 0.7730 - val_loss: 0.4853 - val_acc: 0.8460\n",
      "Epoch 50/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.5499 - acc: 0.7705 - val_loss: 0.4919 - val_acc: 0.8620\n",
      "Epoch 51/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.5233 - acc: 0.7955 - val_loss: 0.4786 - val_acc: 0.8410\n",
      "Epoch 52/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.5299 - acc: 0.7875 - val_loss: 0.4839 - val_acc: 0.8660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.5258 - acc: 0.7775 - val_loss: 0.4734 - val_acc: 0.8480\n",
      "Epoch 54/800\n",
      "2000/2000 [==============================] - 0s 47us/step - loss: 0.5150 - acc: 0.8015 - val_loss: 0.4717 - val_acc: 0.8580\n",
      "Epoch 55/800\n",
      "2000/2000 [==============================] - 0s 43us/step - loss: 0.5101 - acc: 0.8045 - val_loss: 0.4652 - val_acc: 0.8470\n",
      "Epoch 56/800\n",
      "2000/2000 [==============================] - 0s 80us/step - loss: 0.5103 - acc: 0.8025 - val_loss: 0.4638 - val_acc: 0.8400\n",
      "Epoch 57/800\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 0.5093 - acc: 0.8030 - val_loss: 0.4596 - val_acc: 0.8460\n",
      "Epoch 58/800\n",
      "2000/2000 [==============================] - 0s 66us/step - loss: 0.5102 - acc: 0.7970 - val_loss: 0.4674 - val_acc: 0.8720\n",
      "Epoch 59/800\n",
      "2000/2000 [==============================] - 0s 71us/step - loss: 0.5069 - acc: 0.8015 - val_loss: 0.4544 - val_acc: 0.8470\n",
      "Epoch 60/800\n",
      "2000/2000 [==============================] - 0s 67us/step - loss: 0.5022 - acc: 0.8075 - val_loss: 0.4497 - val_acc: 0.8550\n",
      "Epoch 61/800\n",
      "2000/2000 [==============================] - 0s 60us/step - loss: 0.4867 - acc: 0.8080 - val_loss: 0.4493 - val_acc: 0.8580\n",
      "Epoch 62/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.4877 - acc: 0.8040 - val_loss: 0.4464 - val_acc: 0.8650\n",
      "Epoch 63/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.4860 - acc: 0.8060 - val_loss: 0.4405 - val_acc: 0.8600\n",
      "Epoch 64/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.4872 - acc: 0.8045 - val_loss: 0.4382 - val_acc: 0.8580\n",
      "Epoch 65/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.4841 - acc: 0.8120 - val_loss: 0.4388 - val_acc: 0.8690\n",
      "Epoch 66/800\n",
      "2000/2000 [==============================] - 0s 66us/step - loss: 0.4787 - acc: 0.8165 - val_loss: 0.4345 - val_acc: 0.8620\n",
      "Epoch 67/800\n",
      "2000/2000 [==============================] - 0s 172us/step - loss: 0.4790 - acc: 0.8115 - val_loss: 0.4349 - val_acc: 0.8700\n",
      "Epoch 68/800\n",
      "2000/2000 [==============================] - 0s 195us/step - loss: 0.4784 - acc: 0.8105 - val_loss: 0.4279 - val_acc: 0.8500\n",
      "Epoch 69/800\n",
      "2000/2000 [==============================] - 0s 121us/step - loss: 0.4613 - acc: 0.8155 - val_loss: 0.4243 - val_acc: 0.8560\n",
      "Epoch 70/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.4571 - acc: 0.8300 - val_loss: 0.4220 - val_acc: 0.8540\n",
      "Epoch 71/800\n",
      "2000/2000 [==============================] - 0s 48us/step - loss: 0.4601 - acc: 0.8240 - val_loss: 0.4188 - val_acc: 0.8670\n",
      "Epoch 72/800\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.4632 - acc: 0.8260 - val_loss: 0.4164 - val_acc: 0.8720\n",
      "Epoch 73/800\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 0.4550 - acc: 0.8265 - val_loss: 0.4137 - val_acc: 0.8610\n",
      "Epoch 74/800\n",
      "2000/2000 [==============================] - 0s 113us/step - loss: 0.4468 - acc: 0.8265 - val_loss: 0.4150 - val_acc: 0.8470\n",
      "Epoch 75/800\n",
      "2000/2000 [==============================] - 0s 70us/step - loss: 0.4421 - acc: 0.8365 - val_loss: 0.4079 - val_acc: 0.8590\n",
      "Epoch 76/800\n",
      "2000/2000 [==============================] - 0s 74us/step - loss: 0.4438 - acc: 0.8380 - val_loss: 0.4064 - val_acc: 0.8570\n",
      "Epoch 77/800\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 0.4338 - acc: 0.8380 - val_loss: 0.4032 - val_acc: 0.8760\n",
      "Epoch 78/800\n",
      "2000/2000 [==============================] - 0s 62us/step - loss: 0.4470 - acc: 0.8370 - val_loss: 0.4004 - val_acc: 0.8660\n",
      "Epoch 79/800\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 0.4529 - acc: 0.8260 - val_loss: 0.4163 - val_acc: 0.8850\n",
      "Epoch 80/800\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 0.4357 - acc: 0.8315 - val_loss: 0.3959 - val_acc: 0.8650\n",
      "Epoch 81/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.4330 - acc: 0.8400 - val_loss: 0.3938 - val_acc: 0.8760\n",
      "Epoch 82/800\n",
      "2000/2000 [==============================] - 0s 59us/step - loss: 0.4400 - acc: 0.8390 - val_loss: 0.3972 - val_acc: 0.8480\n",
      "Epoch 83/800\n",
      "2000/2000 [==============================] - 0s 45us/step - loss: 0.4227 - acc: 0.8450 - val_loss: 0.3898 - val_acc: 0.8770\n",
      "Epoch 84/800\n",
      "2000/2000 [==============================] - 0s 45us/step - loss: 0.4234 - acc: 0.8485 - val_loss: 0.3882 - val_acc: 0.8810\n",
      "Epoch 85/800\n",
      "2000/2000 [==============================] - 0s 80us/step - loss: 0.4318 - acc: 0.8395 - val_loss: 0.3843 - val_acc: 0.8690\n",
      "Epoch 86/800\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 0.4126 - acc: 0.8425 - val_loss: 0.3847 - val_acc: 0.8560\n",
      "Epoch 87/800\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.4219 - acc: 0.8375 - val_loss: 0.3794 - val_acc: 0.8690\n",
      "Epoch 88/800\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.4113 - acc: 0.8555 - val_loss: 0.3809 - val_acc: 0.8600\n",
      "Epoch 89/800\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 0.4217 - acc: 0.8435 - val_loss: 0.3752 - val_acc: 0.8740\n",
      "Epoch 90/800\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 0.4072 - acc: 0.8580 - val_loss: 0.3748 - val_acc: 0.8840\n",
      "Epoch 91/800\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 0.4082 - acc: 0.8525 - val_loss: 0.3748 - val_acc: 0.8840\n",
      "Epoch 92/800\n",
      "2000/2000 [==============================] - 0s 53us/step - loss: 0.4038 - acc: 0.8500 - val_loss: 0.3684 - val_acc: 0.8740\n",
      "Epoch 93/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.3965 - acc: 0.8575 - val_loss: 0.3681 - val_acc: 0.8670\n",
      "Epoch 94/800\n",
      "2000/2000 [==============================] - 0s 42us/step - loss: 0.4028 - acc: 0.8555 - val_loss: 0.3648 - val_acc: 0.8690\n",
      "Epoch 95/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.3952 - acc: 0.8585 - val_loss: 0.3659 - val_acc: 0.8650\n",
      "Epoch 96/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.3994 - acc: 0.8535 - val_loss: 0.3613 - val_acc: 0.8740\n",
      "Epoch 97/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.3892 - acc: 0.8560 - val_loss: 0.3599 - val_acc: 0.8670\n",
      "Epoch 98/800\n",
      "2000/2000 [==============================] - 0s 42us/step - loss: 0.3865 - acc: 0.8560 - val_loss: 0.3584 - val_acc: 0.8670\n",
      "Epoch 99/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.3835 - acc: 0.8570 - val_loss: 0.3572 - val_acc: 0.8830\n",
      "Epoch 100/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.3895 - acc: 0.8570 - val_loss: 0.3560 - val_acc: 0.8860\n",
      "Epoch 101/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.3818 - acc: 0.8505 - val_loss: 0.3532 - val_acc: 0.8820\n",
      "Epoch 102/800\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 0.3798 - acc: 0.8595 - val_loss: 0.3508 - val_acc: 0.8720\n",
      "Epoch 103/800\n",
      "2000/2000 [==============================] - 0s 59us/step - loss: 0.3914 - acc: 0.8585 - val_loss: 0.3496 - val_acc: 0.8790\n",
      "Epoch 104/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.3777 - acc: 0.8595 - val_loss: 0.3498 - val_acc: 0.8720\n",
      "Epoch 105/800\n",
      "2000/2000 [==============================] - 0s 49us/step - loss: 0.3772 - acc: 0.8625 - val_loss: 0.3490 - val_acc: 0.8730\n",
      "Epoch 106/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.3714 - acc: 0.8655 - val_loss: 0.3469 - val_acc: 0.8790\n",
      "Epoch 107/800\n",
      "2000/2000 [==============================] - 0s 50us/step - loss: 0.3745 - acc: 0.8670 - val_loss: 0.3453 - val_acc: 0.8730\n",
      "Epoch 108/800\n",
      "2000/2000 [==============================] - 0s 57us/step - loss: 0.3748 - acc: 0.8560 - val_loss: 0.3428 - val_acc: 0.8780\n",
      "Epoch 109/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.3778 - acc: 0.8640 - val_loss: 0.3431 - val_acc: 0.8750\n",
      "Epoch 110/800\n",
      "2000/2000 [==============================] - 0s 51us/step - loss: 0.3725 - acc: 0.8660 - val_loss: 0.3416 - val_acc: 0.8740\n",
      "Epoch 111/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.3691 - acc: 0.8725 - val_loss: 0.3405 - val_acc: 0.8810\n",
      "Epoch 112/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.3663 - acc: 0.8630 - val_loss: 0.3390 - val_acc: 0.8780\n",
      "Epoch 113/800\n",
      "2000/2000 [==============================] - 0s 71us/step - loss: 0.3642 - acc: 0.8775 - val_loss: 0.3395 - val_acc: 0.8770\n",
      "Epoch 114/800\n",
      "2000/2000 [==============================] - 0s 46us/step - loss: 0.3668 - acc: 0.8655 - val_loss: 0.3460 - val_acc: 0.8700\n",
      "Epoch 115/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.3662 - acc: 0.8700 - val_loss: 0.3375 - val_acc: 0.8730\n",
      "Epoch 116/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.3548 - acc: 0.8720 - val_loss: 0.3355 - val_acc: 0.8840\n",
      "Epoch 117/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.3583 - acc: 0.8685 - val_loss: 0.3338 - val_acc: 0.8790\n",
      "Epoch 118/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.3544 - acc: 0.8690 - val_loss: 0.3329 - val_acc: 0.8790\n",
      "Epoch 119/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.3560 - acc: 0.8705 - val_loss: 0.3346 - val_acc: 0.8920\n",
      "Epoch 120/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.3498 - acc: 0.8795 - val_loss: 0.3316 - val_acc: 0.8890\n",
      "Epoch 121/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.3498 - acc: 0.8740 - val_loss: 0.3378 - val_acc: 0.8750\n",
      "Epoch 122/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.3473 - acc: 0.8745 - val_loss: 0.3307 - val_acc: 0.8800\n",
      "Epoch 123/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.3557 - acc: 0.8670 - val_loss: 0.3321 - val_acc: 0.8780\n",
      "Epoch 124/800\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3576 - acc: 0.8710 - val_loss: 0.3283 - val_acc: 0.8770\n",
      "Epoch 125/800\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3470 - acc: 0.8735 - val_loss: 0.3280 - val_acc: 0.8820\n",
      "Epoch 126/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.3554 - acc: 0.8685 - val_loss: 0.3273 - val_acc: 0.8830\n",
      "Epoch 127/800\n",
      "2000/2000 [==============================] - 0s 43us/step - loss: 0.3517 - acc: 0.8705 - val_loss: 0.3262 - val_acc: 0.8780\n",
      "Epoch 128/800\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 0.3425 - acc: 0.8730 - val_loss: 0.3258 - val_acc: 0.8800\n",
      "Epoch 129/800\n",
      "2000/2000 [==============================] - 0s 54us/step - loss: 0.3435 - acc: 0.8745 - val_loss: 0.3251 - val_acc: 0.8880\n",
      "Epoch 130/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.3453 - acc: 0.8745 - val_loss: 0.3286 - val_acc: 0.8800\n",
      "Epoch 131/800\n",
      "2000/2000 [==============================] - 0s 42us/step - loss: 0.3490 - acc: 0.8750 - val_loss: 0.3246 - val_acc: 0.8870\n",
      "Epoch 132/800\n",
      "2000/2000 [==============================] - 0s 42us/step - loss: 0.3426 - acc: 0.8720 - val_loss: 0.3224 - val_acc: 0.8900\n",
      "Epoch 133/800\n",
      "2000/2000 [==============================] - 0s 46us/step - loss: 0.3525 - acc: 0.8715 - val_loss: 0.3219 - val_acc: 0.8870\n",
      "Epoch 134/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.3481 - acc: 0.8720 - val_loss: 0.3247 - val_acc: 0.8970\n",
      "Epoch 135/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.3334 - acc: 0.8885 - val_loss: 0.3197 - val_acc: 0.8930\n",
      "Epoch 136/800\n",
      "2000/2000 [==============================] - 0s 45us/step - loss: 0.3380 - acc: 0.8770 - val_loss: 0.3196 - val_acc: 0.8910\n",
      "Epoch 137/800\n",
      "2000/2000 [==============================] - 0s 57us/step - loss: 0.3398 - acc: 0.8780 - val_loss: 0.3231 - val_acc: 0.8930\n",
      "Epoch 138/800\n",
      "2000/2000 [==============================] - 0s 69us/step - loss: 0.3419 - acc: 0.8720 - val_loss: 0.3184 - val_acc: 0.8920\n",
      "Epoch 139/800\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 0.3356 - acc: 0.8805 - val_loss: 0.3173 - val_acc: 0.8930\n",
      "Epoch 140/800\n",
      "2000/2000 [==============================] - 0s 50us/step - loss: 0.3351 - acc: 0.8790 - val_loss: 0.3165 - val_acc: 0.8930\n",
      "Epoch 141/800\n",
      "2000/2000 [==============================] - 0s 50us/step - loss: 0.3301 - acc: 0.8855 - val_loss: 0.3165 - val_acc: 0.8890\n",
      "Epoch 142/800\n",
      "2000/2000 [==============================] - 0s 50us/step - loss: 0.3354 - acc: 0.8770 - val_loss: 0.3163 - val_acc: 0.8890\n",
      "Epoch 143/800\n",
      "2000/2000 [==============================] - 0s 57us/step - loss: 0.3459 - acc: 0.8730 - val_loss: 0.3167 - val_acc: 0.8840\n",
      "Epoch 144/800\n",
      "2000/2000 [==============================] - 0s 47us/step - loss: 0.3305 - acc: 0.8800 - val_loss: 0.3159 - val_acc: 0.8930\n",
      "Epoch 145/800\n",
      "2000/2000 [==============================] - 0s 52us/step - loss: 0.3475 - acc: 0.8755 - val_loss: 0.3147 - val_acc: 0.8930\n",
      "Epoch 146/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.3337 - acc: 0.8835 - val_loss: 0.3137 - val_acc: 0.8940\n",
      "Epoch 147/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.3216 - acc: 0.8845 - val_loss: 0.3145 - val_acc: 0.8940\n",
      "Epoch 148/800\n",
      "2000/2000 [==============================] - 0s 74us/step - loss: 0.3344 - acc: 0.8795 - val_loss: 0.3152 - val_acc: 0.8850\n",
      "Epoch 149/800\n",
      "2000/2000 [==============================] - 0s 78us/step - loss: 0.3253 - acc: 0.8810 - val_loss: 0.3123 - val_acc: 0.8900\n",
      "Epoch 150/800\n",
      "2000/2000 [==============================] - 0s 47us/step - loss: 0.3408 - acc: 0.8705 - val_loss: 0.3162 - val_acc: 0.8830\n",
      "Epoch 151/800\n",
      "2000/2000 [==============================] - 0s 49us/step - loss: 0.3256 - acc: 0.8810 - val_loss: 0.3113 - val_acc: 0.8940\n",
      "Epoch 152/800\n",
      "2000/2000 [==============================] - 0s 57us/step - loss: 0.3292 - acc: 0.8810 - val_loss: 0.3107 - val_acc: 0.8930\n",
      "Epoch 153/800\n",
      "2000/2000 [==============================] - 0s 55us/step - loss: 0.3266 - acc: 0.8825 - val_loss: 0.3104 - val_acc: 0.8960\n",
      "Epoch 154/800\n",
      "2000/2000 [==============================] - 0s 54us/step - loss: 0.3401 - acc: 0.8770 - val_loss: 0.3127 - val_acc: 0.8960\n",
      "Epoch 155/800\n",
      "2000/2000 [==============================] - 0s 47us/step - loss: 0.3351 - acc: 0.8745 - val_loss: 0.3090 - val_acc: 0.8940\n",
      "Epoch 156/800\n",
      "2000/2000 [==============================] - 0s 50us/step - loss: 0.3250 - acc: 0.8830 - val_loss: 0.3083 - val_acc: 0.8940\n",
      "Epoch 157/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.3300 - acc: 0.8760 - val_loss: 0.3087 - val_acc: 0.8960\n",
      "Epoch 158/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.3253 - acc: 0.8830 - val_loss: 0.3074 - val_acc: 0.8940\n",
      "Epoch 159/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.3278 - acc: 0.8830 - val_loss: 0.3075 - val_acc: 0.8950\n",
      "Epoch 160/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.3212 - acc: 0.8830 - val_loss: 0.3065 - val_acc: 0.8930\n",
      "Epoch 161/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.3260 - acc: 0.8745 - val_loss: 0.3063 - val_acc: 0.8950\n",
      "Epoch 162/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.3239 - acc: 0.8810 - val_loss: 0.3072 - val_acc: 0.8960\n",
      "Epoch 163/800\n",
      "2000/2000 [==============================] - 0s 52us/step - loss: 0.3187 - acc: 0.8845 - val_loss: 0.3055 - val_acc: 0.8950\n",
      "Epoch 164/800\n",
      "2000/2000 [==============================] - 0s 43us/step - loss: 0.3170 - acc: 0.8810 - val_loss: 0.3045 - val_acc: 0.8940\n",
      "Epoch 165/800\n",
      "2000/2000 [==============================] - 0s 57us/step - loss: 0.3323 - acc: 0.8770 - val_loss: 0.3062 - val_acc: 0.8950\n",
      "Epoch 166/800\n",
      "2000/2000 [==============================] - 0s 53us/step - loss: 0.3201 - acc: 0.8845 - val_loss: 0.3040 - val_acc: 0.8970\n",
      "Epoch 167/800\n",
      "2000/2000 [==============================] - 0s 43us/step - loss: 0.3194 - acc: 0.8820 - val_loss: 0.3037 - val_acc: 0.8970\n",
      "Epoch 168/800\n",
      "2000/2000 [==============================] - 0s 48us/step - loss: 0.3155 - acc: 0.8895 - val_loss: 0.3036 - val_acc: 0.8950\n",
      "Epoch 169/800\n",
      "2000/2000 [==============================] - 0s 70us/step - loss: 0.3254 - acc: 0.8805 - val_loss: 0.3044 - val_acc: 0.8990\n",
      "Epoch 170/800\n",
      "2000/2000 [==============================] - 0s 67us/step - loss: 0.3238 - acc: 0.8840 - val_loss: 0.3033 - val_acc: 0.8940\n",
      "Epoch 171/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.3319 - acc: 0.8795 - val_loss: 0.3033 - val_acc: 0.8950\n",
      "Epoch 172/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.3181 - acc: 0.8860 - val_loss: 0.3075 - val_acc: 0.8850\n",
      "Epoch 173/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.3324 - acc: 0.8820 - val_loss: 0.3019 - val_acc: 0.8930\n",
      "Epoch 174/800\n",
      "2000/2000 [==============================] - 0s 48us/step - loss: 0.3268 - acc: 0.8780 - val_loss: 0.3027 - val_acc: 0.8930\n",
      "Epoch 175/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.3251 - acc: 0.8820 - val_loss: 0.3016 - val_acc: 0.8980\n",
      "Epoch 176/800\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 0.3197 - acc: 0.8850 - val_loss: 0.3021 - val_acc: 0.8900\n",
      "Epoch 177/800\n",
      "2000/2000 [==============================] - 0s 52us/step - loss: 0.3171 - acc: 0.8815 - val_loss: 0.3016 - val_acc: 0.8970\n",
      "Epoch 178/800\n",
      "2000/2000 [==============================] - 0s 43us/step - loss: 0.3179 - acc: 0.8830 - val_loss: 0.2999 - val_acc: 0.8980\n",
      "Epoch 179/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.3160 - acc: 0.8910 - val_loss: 0.3005 - val_acc: 0.8940\n",
      "Epoch 180/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.3189 - acc: 0.8850 - val_loss: 0.3015 - val_acc: 0.8960\n",
      "Epoch 181/800\n",
      "2000/2000 [==============================] - 0s 50us/step - loss: 0.3150 - acc: 0.8865 - val_loss: 0.2996 - val_acc: 0.8960\n",
      "Epoch 182/800\n",
      "2000/2000 [==============================] - 0s 42us/step - loss: 0.3170 - acc: 0.8880 - val_loss: 0.2999 - val_acc: 0.8980\n",
      "Epoch 183/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.3197 - acc: 0.8880 - val_loss: 0.3007 - val_acc: 0.8960\n",
      "Epoch 184/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.3188 - acc: 0.8830 - val_loss: 0.2997 - val_acc: 0.8950\n",
      "Epoch 185/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.3289 - acc: 0.8810 - val_loss: 0.2990 - val_acc: 0.8970\n",
      "Epoch 186/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.3143 - acc: 0.8845 - val_loss: 0.2987 - val_acc: 0.8950\n",
      "Epoch 187/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.3092 - acc: 0.8850 - val_loss: 0.2988 - val_acc: 0.8950\n",
      "Epoch 188/800\n",
      "2000/2000 [==============================] - 0s 54us/step - loss: 0.3129 - acc: 0.8890 - val_loss: 0.2998 - val_acc: 0.9000\n",
      "Epoch 189/800\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 0.3116 - acc: 0.8880 - val_loss: 0.2972 - val_acc: 0.8940\n",
      "Epoch 190/800\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 0.3114 - acc: 0.8870 - val_loss: 0.2966 - val_acc: 0.8980\n",
      "Epoch 191/800\n",
      "2000/2000 [==============================] - 0s 66us/step - loss: 0.3105 - acc: 0.8820 - val_loss: 0.2964 - val_acc: 0.8990\n",
      "Epoch 192/800\n",
      "2000/2000 [==============================] - 0s 67us/step - loss: 0.3161 - acc: 0.8860 - val_loss: 0.2965 - val_acc: 0.8970\n",
      "Epoch 193/800\n",
      "2000/2000 [==============================] - 0s 59us/step - loss: 0.3125 - acc: 0.8865 - val_loss: 0.2972 - val_acc: 0.8960\n",
      "Epoch 194/800\n",
      "2000/2000 [==============================] - 0s 45us/step - loss: 0.3080 - acc: 0.8875 - val_loss: 0.2981 - val_acc: 0.8910\n",
      "Epoch 195/800\n",
      "2000/2000 [==============================] - 0s 48us/step - loss: 0.3125 - acc: 0.8825 - val_loss: 0.2949 - val_acc: 0.8950\n",
      "Epoch 196/800\n",
      "2000/2000 [==============================] - 0s 58us/step - loss: 0.3174 - acc: 0.8820 - val_loss: 0.3004 - val_acc: 0.8900\n",
      "Epoch 197/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.3058 - acc: 0.8900 - val_loss: 0.2946 - val_acc: 0.8980\n",
      "Epoch 198/800\n",
      "2000/2000 [==============================] - 0s 61us/step - loss: 0.3089 - acc: 0.8905 - val_loss: 0.2954 - val_acc: 0.8950\n",
      "Epoch 199/800\n",
      "2000/2000 [==============================] - 0s 72us/step - loss: 0.3056 - acc: 0.8930 - val_loss: 0.2952 - val_acc: 0.8970\n",
      "Epoch 200/800\n",
      "2000/2000 [==============================] - 0s 52us/step - loss: 0.3153 - acc: 0.8875 - val_loss: 0.2973 - val_acc: 0.8930\n",
      "Epoch 201/800\n",
      "2000/2000 [==============================] - 0s 56us/step - loss: 0.3067 - acc: 0.8905 - val_loss: 0.2955 - val_acc: 0.8930\n",
      "Epoch 202/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.3101 - acc: 0.8815 - val_loss: 0.2946 - val_acc: 0.8970\n",
      "Epoch 203/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.3138 - acc: 0.8820 - val_loss: 0.2944 - val_acc: 0.8980\n",
      "Epoch 204/800\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.3150 - acc: 0.8865 - val_loss: 0.2936 - val_acc: 0.8940\n",
      "Epoch 205/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.3080 - acc: 0.8930 - val_loss: 0.2936 - val_acc: 0.8980\n",
      "Epoch 206/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.3229 - acc: 0.8885 - val_loss: 0.2936 - val_acc: 0.8970\n",
      "Epoch 207/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.3027 - acc: 0.8925 - val_loss: 0.2942 - val_acc: 0.8960\n",
      "Epoch 208/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.2982 - acc: 0.8885 - val_loss: 0.2932 - val_acc: 0.8950\n",
      "Epoch 209/800\n",
      "2000/2000 [==============================] - 0s 50us/step - loss: 0.3071 - acc: 0.8900 - val_loss: 0.2917 - val_acc: 0.8950\n",
      "Epoch 210/800\n",
      "2000/2000 [==============================] - 0s 45us/step - loss: 0.2962 - acc: 0.8910 - val_loss: 0.2920 - val_acc: 0.8960\n",
      "Epoch 211/800\n",
      "2000/2000 [==============================] - 0s 51us/step - loss: 0.3084 - acc: 0.8860 - val_loss: 0.2929 - val_acc: 0.8940\n",
      "Epoch 212/800\n",
      "2000/2000 [==============================] - 0s 56us/step - loss: 0.3108 - acc: 0.8850 - val_loss: 0.2909 - val_acc: 0.8990\n",
      "Epoch 213/800\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 0.3041 - acc: 0.8920 - val_loss: 0.2909 - val_acc: 0.9000\n",
      "Epoch 214/800\n",
      "2000/2000 [==============================] - 0s 76us/step - loss: 0.3065 - acc: 0.8920 - val_loss: 0.2913 - val_acc: 0.8960\n",
      "Epoch 215/800\n",
      "2000/2000 [==============================] - 0s 50us/step - loss: 0.3124 - acc: 0.8850 - val_loss: 0.2905 - val_acc: 0.8970\n",
      "Epoch 216/800\n",
      "2000/2000 [==============================] - 0s 53us/step - loss: 0.3059 - acc: 0.8865 - val_loss: 0.2901 - val_acc: 0.8980\n",
      "Epoch 217/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.3028 - acc: 0.8955 - val_loss: 0.2895 - val_acc: 0.8980\n",
      "Epoch 218/800\n",
      "2000/2000 [==============================] - 0s 48us/step - loss: 0.3118 - acc: 0.8840 - val_loss: 0.2894 - val_acc: 0.8980\n",
      "Epoch 219/800\n",
      "2000/2000 [==============================] - 0s 52us/step - loss: 0.3050 - acc: 0.8875 - val_loss: 0.2893 - val_acc: 0.8980\n",
      "Epoch 220/800\n",
      "2000/2000 [==============================] - 0s 50us/step - loss: 0.2933 - acc: 0.8940 - val_loss: 0.2887 - val_acc: 0.8980\n",
      "Epoch 221/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.2985 - acc: 0.8900 - val_loss: 0.2896 - val_acc: 0.8960\n",
      "Epoch 222/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.3034 - acc: 0.8890 - val_loss: 0.2910 - val_acc: 0.8970\n",
      "Epoch 223/800\n",
      "2000/2000 [==============================] - 0s 63us/step - loss: 0.2989 - acc: 0.8905 - val_loss: 0.2888 - val_acc: 0.8980\n",
      "Epoch 224/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2981 - acc: 0.8925 - val_loss: 0.2888 - val_acc: 0.8980\n",
      "Epoch 225/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.3029 - acc: 0.8870 - val_loss: 0.2882 - val_acc: 0.8960\n",
      "Epoch 226/800\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3040 - acc: 0.8880 - val_loss: 0.2897 - val_acc: 0.9030\n",
      "Epoch 227/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.3019 - acc: 0.8885 - val_loss: 0.2879 - val_acc: 0.8970\n",
      "Epoch 228/800\n",
      "2000/2000 [==============================] - 0s 44us/step - loss: 0.3050 - acc: 0.8880 - val_loss: 0.2882 - val_acc: 0.9040\n",
      "Epoch 229/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2932 - acc: 0.8925 - val_loss: 0.2874 - val_acc: 0.9010\n",
      "Epoch 230/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2974 - acc: 0.8910 - val_loss: 0.2894 - val_acc: 0.8960\n",
      "Epoch 231/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.2979 - acc: 0.8895 - val_loss: 0.2902 - val_acc: 0.8950\n",
      "Epoch 232/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.3018 - acc: 0.8925 - val_loss: 0.2871 - val_acc: 0.9020\n",
      "Epoch 233/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2999 - acc: 0.8890 - val_loss: 0.2864 - val_acc: 0.8990\n",
      "Epoch 234/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.2979 - acc: 0.8915 - val_loss: 0.2880 - val_acc: 0.8980\n",
      "Epoch 235/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.3038 - acc: 0.8940 - val_loss: 0.2876 - val_acc: 0.8970\n",
      "Epoch 236/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.2966 - acc: 0.8940 - val_loss: 0.2887 - val_acc: 0.8970\n",
      "Epoch 237/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.3010 - acc: 0.8895 - val_loss: 0.2852 - val_acc: 0.9010\n",
      "Epoch 238/800\n",
      "2000/2000 [==============================] - 0s 42us/step - loss: 0.3024 - acc: 0.8940 - val_loss: 0.2857 - val_acc: 0.9010\n",
      "Epoch 239/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2916 - acc: 0.8945 - val_loss: 0.2851 - val_acc: 0.9000\n",
      "Epoch 240/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2958 - acc: 0.8920 - val_loss: 0.2864 - val_acc: 0.8980\n",
      "Epoch 241/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.2867 - acc: 0.8985 - val_loss: 0.2848 - val_acc: 0.9000\n",
      "Epoch 242/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2940 - acc: 0.8970 - val_loss: 0.2873 - val_acc: 0.8970\n",
      "Epoch 243/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2960 - acc: 0.8925 - val_loss: 0.2850 - val_acc: 0.9040\n",
      "Epoch 244/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.2981 - acc: 0.8930 - val_loss: 0.2879 - val_acc: 0.8970\n",
      "Epoch 245/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.2908 - acc: 0.8975 - val_loss: 0.2845 - val_acc: 0.9010\n",
      "Epoch 246/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.2987 - acc: 0.8935 - val_loss: 0.2844 - val_acc: 0.9000\n",
      "Epoch 247/800\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2955 - acc: 0.8915 - val_loss: 0.2843 - val_acc: 0.9010\n",
      "Epoch 248/800\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2983 - acc: 0.8930 - val_loss: 0.2857 - val_acc: 0.8970\n",
      "Epoch 249/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.2964 - acc: 0.8910 - val_loss: 0.2864 - val_acc: 0.8980\n",
      "Epoch 250/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.2996 - acc: 0.8935 - val_loss: 0.2844 - val_acc: 0.9010\n",
      "Epoch 251/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.3022 - acc: 0.8905 - val_loss: 0.2833 - val_acc: 0.9000\n",
      "Epoch 252/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.3014 - acc: 0.8895 - val_loss: 0.2877 - val_acc: 0.8960\n",
      "Epoch 253/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2904 - acc: 0.8930 - val_loss: 0.2826 - val_acc: 0.9020\n",
      "Epoch 254/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.2947 - acc: 0.8905 - val_loss: 0.2845 - val_acc: 0.9060\n",
      "Epoch 255/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2892 - acc: 0.8970 - val_loss: 0.2821 - val_acc: 0.9030\n",
      "Epoch 256/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.2982 - acc: 0.8845 - val_loss: 0.2825 - val_acc: 0.9020\n",
      "Epoch 257/800\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2913 - acc: 0.8905 - val_loss: 0.2824 - val_acc: 0.9050\n",
      "Epoch 258/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.2948 - acc: 0.8910 - val_loss: 0.2829 - val_acc: 0.9020\n",
      "Epoch 259/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2929 - acc: 0.8940 - val_loss: 0.2812 - val_acc: 0.9020\n",
      "Epoch 260/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.3030 - acc: 0.8850 - val_loss: 0.2818 - val_acc: 0.9020\n",
      "Epoch 261/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.2946 - acc: 0.8890 - val_loss: 0.2811 - val_acc: 0.9010\n",
      "Epoch 262/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.3041 - acc: 0.8930 - val_loss: 0.2816 - val_acc: 0.9050\n",
      "Epoch 263/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2923 - acc: 0.8925 - val_loss: 0.2809 - val_acc: 0.9000\n",
      "Epoch 264/800\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2894 - acc: 0.8920 - val_loss: 0.2801 - val_acc: 0.9020\n",
      "Epoch 265/800\n",
      "2000/2000 [==============================] - 0s 43us/step - loss: 0.2960 - acc: 0.8850 - val_loss: 0.2834 - val_acc: 0.8990\n",
      "Epoch 266/800\n",
      "2000/2000 [==============================] - 0s 49us/step - loss: 0.2996 - acc: 0.8885 - val_loss: 0.2804 - val_acc: 0.9010\n",
      "Epoch 267/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2979 - acc: 0.8935 - val_loss: 0.2804 - val_acc: 0.9010\n",
      "Epoch 268/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2951 - acc: 0.8945 - val_loss: 0.2802 - val_acc: 0.9010\n",
      "Epoch 269/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2888 - acc: 0.8955 - val_loss: 0.2806 - val_acc: 0.9070\n",
      "Epoch 270/800\n",
      "2000/2000 [==============================] - 0s 42us/step - loss: 0.2939 - acc: 0.8910 - val_loss: 0.2821 - val_acc: 0.9070\n",
      "Epoch 271/800\n",
      "2000/2000 [==============================] - 0s 49us/step - loss: 0.2887 - acc: 0.8915 - val_loss: 0.2794 - val_acc: 0.9030\n",
      "Epoch 272/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2903 - acc: 0.8915 - val_loss: 0.2792 - val_acc: 0.9030\n",
      "Epoch 273/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.2965 - acc: 0.8910 - val_loss: 0.2814 - val_acc: 0.9080\n",
      "Epoch 274/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2953 - acc: 0.8930 - val_loss: 0.2793 - val_acc: 0.9030\n",
      "Epoch 275/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.2851 - acc: 0.8965 - val_loss: 0.2794 - val_acc: 0.9010\n",
      "Epoch 276/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2888 - acc: 0.8935 - val_loss: 0.2785 - val_acc: 0.9060\n",
      "Epoch 277/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2940 - acc: 0.8915 - val_loss: 0.2785 - val_acc: 0.9070\n",
      "Epoch 278/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.2872 - acc: 0.8985 - val_loss: 0.2787 - val_acc: 0.9020\n",
      "Epoch 279/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2914 - acc: 0.8965 - val_loss: 0.2796 - val_acc: 0.9030\n",
      "Epoch 280/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2873 - acc: 0.8955 - val_loss: 0.2779 - val_acc: 0.9040\n",
      "Epoch 281/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2861 - acc: 0.8975 - val_loss: 0.2789 - val_acc: 0.9020\n",
      "Epoch 282/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2942 - acc: 0.8950 - val_loss: 0.2777 - val_acc: 0.9060\n",
      "Epoch 283/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.3000 - acc: 0.8875 - val_loss: 0.2807 - val_acc: 0.8970\n",
      "Epoch 284/800\n",
      "2000/2000 [==============================] - 0s 48us/step - loss: 0.2813 - acc: 0.9000 - val_loss: 0.2801 - val_acc: 0.8980\n",
      "Epoch 285/800\n",
      "2000/2000 [==============================] - 0s 62us/step - loss: 0.2798 - acc: 0.8975 - val_loss: 0.2779 - val_acc: 0.9020\n",
      "Epoch 286/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2840 - acc: 0.8970 - val_loss: 0.2778 - val_acc: 0.9020\n",
      "Epoch 287/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.2897 - acc: 0.8950 - val_loss: 0.2769 - val_acc: 0.9030\n",
      "Epoch 288/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2874 - acc: 0.8920 - val_loss: 0.2771 - val_acc: 0.9090\n",
      "Epoch 289/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2831 - acc: 0.9040 - val_loss: 0.2782 - val_acc: 0.9080\n",
      "Epoch 290/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.2907 - acc: 0.8945 - val_loss: 0.2784 - val_acc: 0.9000\n",
      "Epoch 291/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2890 - acc: 0.8940 - val_loss: 0.2764 - val_acc: 0.9030\n",
      "Epoch 292/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2909 - acc: 0.8980 - val_loss: 0.2770 - val_acc: 0.9010\n",
      "Epoch 293/800\n",
      "2000/2000 [==============================] - 0s 51us/step - loss: 0.2883 - acc: 0.8915 - val_loss: 0.2779 - val_acc: 0.8990\n",
      "Epoch 294/800\n",
      "2000/2000 [==============================] - 0s 49us/step - loss: 0.2865 - acc: 0.8965 - val_loss: 0.2763 - val_acc: 0.9030\n",
      "Epoch 295/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.2890 - acc: 0.8900 - val_loss: 0.2757 - val_acc: 0.9050\n",
      "Epoch 296/800\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2829 - acc: 0.8980 - val_loss: 0.2769 - val_acc: 0.9010\n",
      "Epoch 297/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2868 - acc: 0.8975 - val_loss: 0.2761 - val_acc: 0.9040\n",
      "Epoch 298/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2800 - acc: 0.8975 - val_loss: 0.2769 - val_acc: 0.9000\n",
      "Epoch 299/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.2889 - acc: 0.8955 - val_loss: 0.2761 - val_acc: 0.9050\n",
      "Epoch 300/800\n",
      "2000/2000 [==============================] - 0s 66us/step - loss: 0.2826 - acc: 0.9005 - val_loss: 0.2757 - val_acc: 0.9050\n",
      "Epoch 301/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.2834 - acc: 0.9000 - val_loss: 0.2761 - val_acc: 0.9080\n",
      "Epoch 302/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2815 - acc: 0.8930 - val_loss: 0.2755 - val_acc: 0.9040\n",
      "Epoch 303/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2884 - acc: 0.8960 - val_loss: 0.2752 - val_acc: 0.9030\n",
      "Epoch 304/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2868 - acc: 0.8970 - val_loss: 0.2752 - val_acc: 0.9010\n",
      "Epoch 305/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2810 - acc: 0.8970 - val_loss: 0.2748 - val_acc: 0.9020\n",
      "Epoch 306/800\n",
      "2000/2000 [==============================] - 0s 57us/step - loss: 0.2850 - acc: 0.8985 - val_loss: 0.2767 - val_acc: 0.8990\n",
      "Epoch 307/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2868 - acc: 0.8960 - val_loss: 0.2745 - val_acc: 0.9040\n",
      "Epoch 308/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2786 - acc: 0.9005 - val_loss: 0.2741 - val_acc: 0.9060\n",
      "Epoch 309/800\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2831 - acc: 0.8995 - val_loss: 0.2750 - val_acc: 0.9020\n",
      "Epoch 310/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.2886 - acc: 0.8960 - val_loss: 0.2752 - val_acc: 0.9030\n",
      "Epoch 311/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2847 - acc: 0.8960 - val_loss: 0.2759 - val_acc: 0.9000\n",
      "Epoch 312/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2825 - acc: 0.8960 - val_loss: 0.2741 - val_acc: 0.9040\n",
      "Epoch 313/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2833 - acc: 0.8930 - val_loss: 0.2738 - val_acc: 0.9070\n",
      "Epoch 314/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2690 - acc: 0.9055 - val_loss: 0.2734 - val_acc: 0.9080\n",
      "Epoch 315/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.2866 - acc: 0.8945 - val_loss: 0.2744 - val_acc: 0.9050\n",
      "Epoch 316/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2798 - acc: 0.8980 - val_loss: 0.2733 - val_acc: 0.9060\n",
      "Epoch 317/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2859 - acc: 0.8945 - val_loss: 0.2737 - val_acc: 0.9030\n",
      "Epoch 318/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2856 - acc: 0.8945 - val_loss: 0.2731 - val_acc: 0.9080\n",
      "Epoch 319/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2733 - acc: 0.9030 - val_loss: 0.2735 - val_acc: 0.9050\n",
      "Epoch 320/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2892 - acc: 0.8900 - val_loss: 0.2724 - val_acc: 0.9060\n",
      "Epoch 321/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2824 - acc: 0.9020 - val_loss: 0.2755 - val_acc: 0.9010\n",
      "Epoch 322/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2849 - acc: 0.8990 - val_loss: 0.2751 - val_acc: 0.9000\n",
      "Epoch 323/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2845 - acc: 0.8970 - val_loss: 0.2738 - val_acc: 0.9020\n",
      "Epoch 324/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2856 - acc: 0.8980 - val_loss: 0.2729 - val_acc: 0.9060\n",
      "Epoch 325/800\n",
      "2000/2000 [==============================] - 0s 43us/step - loss: 0.2743 - acc: 0.9030 - val_loss: 0.2725 - val_acc: 0.9050\n",
      "Epoch 326/800\n",
      "2000/2000 [==============================] - 0s 59us/step - loss: 0.2829 - acc: 0.8970 - val_loss: 0.2721 - val_acc: 0.9060\n",
      "Epoch 327/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.2734 - acc: 0.9020 - val_loss: 0.2721 - val_acc: 0.9060\n",
      "Epoch 328/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2825 - acc: 0.8965 - val_loss: 0.2718 - val_acc: 0.9070\n",
      "Epoch 329/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.2847 - acc: 0.8940 - val_loss: 0.2724 - val_acc: 0.9090\n",
      "Epoch 330/800\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2768 - acc: 0.9010 - val_loss: 0.2739 - val_acc: 0.8990\n",
      "Epoch 331/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.2821 - acc: 0.8980 - val_loss: 0.2716 - val_acc: 0.9080\n",
      "Epoch 332/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2839 - acc: 0.8960 - val_loss: 0.2710 - val_acc: 0.9070\n",
      "Epoch 333/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.2734 - acc: 0.9020 - val_loss: 0.2729 - val_acc: 0.9030\n",
      "Epoch 334/800\n",
      "2000/2000 [==============================] - 0s 51us/step - loss: 0.2813 - acc: 0.8995 - val_loss: 0.2708 - val_acc: 0.9080\n",
      "Epoch 335/800\n",
      "2000/2000 [==============================] - 0s 46us/step - loss: 0.2810 - acc: 0.8940 - val_loss: 0.2708 - val_acc: 0.9090\n",
      "Epoch 336/800\n",
      "2000/2000 [==============================] - 0s 45us/step - loss: 0.2806 - acc: 0.8965 - val_loss: 0.2744 - val_acc: 0.9000\n",
      "Epoch 337/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2854 - acc: 0.8960 - val_loss: 0.2715 - val_acc: 0.9080\n",
      "Epoch 338/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2773 - acc: 0.9000 - val_loss: 0.2710 - val_acc: 0.9060\n",
      "Epoch 339/800\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2770 - acc: 0.8990 - val_loss: 0.2701 - val_acc: 0.9090\n",
      "Epoch 340/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.2806 - acc: 0.8985 - val_loss: 0.2701 - val_acc: 0.9090\n",
      "Epoch 341/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2796 - acc: 0.9000 - val_loss: 0.2736 - val_acc: 0.8980\n",
      "Epoch 342/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2733 - acc: 0.8965 - val_loss: 0.2702 - val_acc: 0.9100\n",
      "Epoch 343/800\n",
      "2000/2000 [==============================] - 0s 65us/step - loss: 0.2835 - acc: 0.8940 - val_loss: 0.2712 - val_acc: 0.9040\n",
      "Epoch 344/800\n",
      "2000/2000 [==============================] - 0s 68us/step - loss: 0.2822 - acc: 0.8990 - val_loss: 0.2695 - val_acc: 0.9090\n",
      "Epoch 345/800\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 0.2787 - acc: 0.8975 - val_loss: 0.2706 - val_acc: 0.9090\n",
      "Epoch 346/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.2756 - acc: 0.9005 - val_loss: 0.2696 - val_acc: 0.9070\n",
      "Epoch 347/800\n",
      "2000/2000 [==============================] - 0s 53us/step - loss: 0.2827 - acc: 0.8960 - val_loss: 0.2689 - val_acc: 0.9080\n",
      "Epoch 348/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.2761 - acc: 0.9020 - val_loss: 0.2692 - val_acc: 0.9100\n",
      "Epoch 349/800\n",
      "2000/2000 [==============================] - 0s 43us/step - loss: 0.2748 - acc: 0.8995 - val_loss: 0.2694 - val_acc: 0.9070\n",
      "Epoch 350/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2787 - acc: 0.8965 - val_loss: 0.2694 - val_acc: 0.9080\n",
      "Epoch 351/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.2732 - acc: 0.8980 - val_loss: 0.2687 - val_acc: 0.9100\n",
      "Epoch 352/800\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2747 - acc: 0.8985 - val_loss: 0.2692 - val_acc: 0.9090\n",
      "Epoch 353/800\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2713 - acc: 0.9010 - val_loss: 0.2691 - val_acc: 0.9100\n",
      "Epoch 354/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.2665 - acc: 0.9005 - val_loss: 0.2714 - val_acc: 0.9050\n",
      "Epoch 355/800\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 0.2681 - acc: 0.9025 - val_loss: 0.2684 - val_acc: 0.9090\n",
      "Epoch 356/800\n",
      "2000/2000 [==============================] - 0s 43us/step - loss: 0.2772 - acc: 0.8970 - val_loss: 0.2699 - val_acc: 0.9040\n",
      "Epoch 357/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2845 - acc: 0.8990 - val_loss: 0.2683 - val_acc: 0.9110\n",
      "Epoch 358/800\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2716 - acc: 0.9005 - val_loss: 0.2713 - val_acc: 0.9050\n",
      "Epoch 359/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2753 - acc: 0.9010 - val_loss: 0.2720 - val_acc: 0.9010\n",
      "Epoch 360/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.2771 - acc: 0.9010 - val_loss: 0.2691 - val_acc: 0.9080\n",
      "Epoch 361/800\n",
      "2000/2000 [==============================] - 0s 47us/step - loss: 0.2745 - acc: 0.9025 - val_loss: 0.2702 - val_acc: 0.9020\n",
      "Epoch 362/800\n",
      "2000/2000 [==============================] - 0s 46us/step - loss: 0.2719 - acc: 0.9005 - val_loss: 0.2678 - val_acc: 0.9080\n",
      "Epoch 363/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.2655 - acc: 0.9045 - val_loss: 0.2676 - val_acc: 0.9110\n",
      "Epoch 364/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2717 - acc: 0.9025 - val_loss: 0.2706 - val_acc: 0.9040\n",
      "Epoch 365/800\n",
      "2000/2000 [==============================] - 0s 53us/step - loss: 0.2774 - acc: 0.9000 - val_loss: 0.2684 - val_acc: 0.9070\n",
      "Epoch 366/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2796 - acc: 0.9030 - val_loss: 0.2674 - val_acc: 0.9120\n",
      "Epoch 367/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2745 - acc: 0.9020 - val_loss: 0.2672 - val_acc: 0.9080\n",
      "Epoch 368/800\n",
      "2000/2000 [==============================] - 0s 68us/step - loss: 0.2735 - acc: 0.9015 - val_loss: 0.2679 - val_acc: 0.9090\n",
      "Epoch 369/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.2780 - acc: 0.8935 - val_loss: 0.2693 - val_acc: 0.9060\n",
      "Epoch 370/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2776 - acc: 0.9055 - val_loss: 0.2668 - val_acc: 0.9110\n",
      "Epoch 371/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.2719 - acc: 0.9045 - val_loss: 0.2667 - val_acc: 0.9090\n",
      "Epoch 372/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2692 - acc: 0.9035 - val_loss: 0.2683 - val_acc: 0.9050\n",
      "Epoch 373/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2726 - acc: 0.8965 - val_loss: 0.2664 - val_acc: 0.9070\n",
      "Epoch 374/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2653 - acc: 0.9060 - val_loss: 0.2658 - val_acc: 0.9100\n",
      "Epoch 375/800\n",
      "2000/2000 [==============================] - 0s 43us/step - loss: 0.2776 - acc: 0.8975 - val_loss: 0.2661 - val_acc: 0.9110\n",
      "Epoch 376/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2730 - acc: 0.8980 - val_loss: 0.2675 - val_acc: 0.9070\n",
      "Epoch 377/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2682 - acc: 0.9005 - val_loss: 0.2659 - val_acc: 0.9090\n",
      "Epoch 378/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2774 - acc: 0.9020 - val_loss: 0.2653 - val_acc: 0.9120\n",
      "Epoch 379/800\n",
      "2000/2000 [==============================] - 0s 47us/step - loss: 0.2691 - acc: 0.8990 - val_loss: 0.2657 - val_acc: 0.9080\n",
      "Epoch 380/800\n",
      "2000/2000 [==============================] - 0s 67us/step - loss: 0.2745 - acc: 0.9045 - val_loss: 0.2660 - val_acc: 0.9110\n",
      "Epoch 381/800\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 0.2676 - acc: 0.9055 - val_loss: 0.2674 - val_acc: 0.9090\n",
      "Epoch 382/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2658 - acc: 0.9050 - val_loss: 0.2658 - val_acc: 0.9080\n",
      "Epoch 383/800\n",
      "2000/2000 [==============================] - 0s 54us/step - loss: 0.2803 - acc: 0.8955 - val_loss: 0.2664 - val_acc: 0.9060\n",
      "Epoch 384/800\n",
      "2000/2000 [==============================] - 0s 62us/step - loss: 0.2730 - acc: 0.9045 - val_loss: 0.2664 - val_acc: 0.9080\n",
      "Epoch 385/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.2677 - acc: 0.9045 - val_loss: 0.2652 - val_acc: 0.9080\n",
      "Epoch 386/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.2673 - acc: 0.9060 - val_loss: 0.2681 - val_acc: 0.9050\n",
      "Epoch 387/800\n",
      "2000/2000 [==============================] - 0s 67us/step - loss: 0.2808 - acc: 0.8960 - val_loss: 0.2657 - val_acc: 0.9090\n",
      "Epoch 388/800\n",
      "2000/2000 [==============================] - 0s 57us/step - loss: 0.2684 - acc: 0.9060 - val_loss: 0.2643 - val_acc: 0.9110\n",
      "Epoch 389/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.2711 - acc: 0.9055 - val_loss: 0.2639 - val_acc: 0.9120\n",
      "Epoch 390/800\n",
      "2000/2000 [==============================] - 0s 72us/step - loss: 0.2672 - acc: 0.9005 - val_loss: 0.2664 - val_acc: 0.9050\n",
      "Epoch 391/800\n",
      "2000/2000 [==============================] - 0s 73us/step - loss: 0.2676 - acc: 0.9065 - val_loss: 0.2654 - val_acc: 0.9070\n",
      "Epoch 392/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2792 - acc: 0.8980 - val_loss: 0.2658 - val_acc: 0.9050\n",
      "Epoch 393/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2704 - acc: 0.9030 - val_loss: 0.2645 - val_acc: 0.9110\n",
      "Epoch 394/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2692 - acc: 0.9020 - val_loss: 0.2642 - val_acc: 0.9100\n",
      "Epoch 395/800\n",
      "2000/2000 [==============================] - 0s 51us/step - loss: 0.2651 - acc: 0.8995 - val_loss: 0.2646 - val_acc: 0.9070\n",
      "Epoch 396/800\n",
      "2000/2000 [==============================] - 0s 61us/step - loss: 0.2683 - acc: 0.9055 - val_loss: 0.2645 - val_acc: 0.9060\n",
      "Epoch 397/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2653 - acc: 0.9060 - val_loss: 0.2665 - val_acc: 0.9070\n",
      "Epoch 398/800\n",
      "2000/2000 [==============================] - 0s 46us/step - loss: 0.2696 - acc: 0.8955 - val_loss: 0.2652 - val_acc: 0.9080\n",
      "Epoch 399/800\n",
      "2000/2000 [==============================] - 0s 45us/step - loss: 0.2720 - acc: 0.9000 - val_loss: 0.2645 - val_acc: 0.9070\n",
      "Epoch 400/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2777 - acc: 0.8980 - val_loss: 0.2643 - val_acc: 0.9070\n",
      "Epoch 401/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.2661 - acc: 0.9035 - val_loss: 0.2640 - val_acc: 0.9120\n",
      "Epoch 402/800\n",
      "2000/2000 [==============================] - 0s 67us/step - loss: 0.2671 - acc: 0.9035 - val_loss: 0.2636 - val_acc: 0.9080\n",
      "Epoch 403/800\n",
      "2000/2000 [==============================] - 0s 58us/step - loss: 0.2670 - acc: 0.9045 - val_loss: 0.2631 - val_acc: 0.9060\n",
      "Epoch 404/800\n",
      "2000/2000 [==============================] - 0s 75us/step - loss: 0.2687 - acc: 0.9005 - val_loss: 0.2636 - val_acc: 0.9070\n",
      "Epoch 405/800\n",
      "2000/2000 [==============================] - 0s 48us/step - loss: 0.2699 - acc: 0.9050 - val_loss: 0.2623 - val_acc: 0.9100\n",
      "Epoch 406/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.2655 - acc: 0.9025 - val_loss: 0.2637 - val_acc: 0.9090\n",
      "Epoch 407/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2684 - acc: 0.9055 - val_loss: 0.2637 - val_acc: 0.9050\n",
      "Epoch 408/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2691 - acc: 0.8995 - val_loss: 0.2629 - val_acc: 0.9080\n",
      "Epoch 409/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2684 - acc: 0.8965 - val_loss: 0.2620 - val_acc: 0.9110\n",
      "Epoch 410/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.2622 - acc: 0.9025 - val_loss: 0.2631 - val_acc: 0.9100\n",
      "Epoch 411/800\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2644 - acc: 0.9070 - val_loss: 0.2631 - val_acc: 0.9050\n",
      "Epoch 412/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2701 - acc: 0.9010 - val_loss: 0.2616 - val_acc: 0.9100\n",
      "Epoch 413/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.2656 - acc: 0.9050 - val_loss: 0.2623 - val_acc: 0.9080\n",
      "Epoch 414/800\n",
      "2000/2000 [==============================] - 0s 80us/step - loss: 0.2624 - acc: 0.9055 - val_loss: 0.2620 - val_acc: 0.9080\n",
      "Epoch 415/800\n",
      "2000/2000 [==============================] - 0s 47us/step - loss: 0.2655 - acc: 0.9060 - val_loss: 0.2645 - val_acc: 0.9050\n",
      "Epoch 416/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.2724 - acc: 0.9000 - val_loss: 0.2626 - val_acc: 0.9070\n",
      "Epoch 417/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2695 - acc: 0.9025 - val_loss: 0.2611 - val_acc: 0.9080\n",
      "Epoch 418/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2713 - acc: 0.9040 - val_loss: 0.2618 - val_acc: 0.9110\n",
      "Epoch 419/800\n",
      "2000/2000 [==============================] - 0s 43us/step - loss: 0.2599 - acc: 0.9050 - val_loss: 0.2644 - val_acc: 0.9060\n",
      "Epoch 420/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2634 - acc: 0.8995 - val_loss: 0.2611 - val_acc: 0.9090\n",
      "Epoch 421/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.2672 - acc: 0.9020 - val_loss: 0.2609 - val_acc: 0.9090\n",
      "Epoch 422/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2612 - acc: 0.9090 - val_loss: 0.2613 - val_acc: 0.9070\n",
      "Epoch 423/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2695 - acc: 0.9040 - val_loss: 0.2605 - val_acc: 0.9100\n",
      "Epoch 424/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2610 - acc: 0.9020 - val_loss: 0.2612 - val_acc: 0.9080\n",
      "Epoch 425/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2694 - acc: 0.9005 - val_loss: 0.2608 - val_acc: 0.9110\n",
      "Epoch 426/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2637 - acc: 0.9040 - val_loss: 0.2605 - val_acc: 0.9090\n",
      "Epoch 427/800\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2545 - acc: 0.9095 - val_loss: 0.2583 - val_acc: 0.9130\n",
      "Epoch 428/800\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.2683 - acc: 0.8990 - val_loss: 0.2622 - val_acc: 0.9050\n",
      "Epoch 429/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2739 - acc: 0.9020 - val_loss: 0.2609 - val_acc: 0.9090\n",
      "Epoch 430/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.2690 - acc: 0.9030 - val_loss: 0.2593 - val_acc: 0.9130\n",
      "Epoch 431/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.2629 - acc: 0.9025 - val_loss: 0.2588 - val_acc: 0.9130\n",
      "Epoch 432/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.2689 - acc: 0.9025 - val_loss: 0.2601 - val_acc: 0.9080\n",
      "Epoch 433/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2599 - acc: 0.9075 - val_loss: 0.2622 - val_acc: 0.9060\n",
      "Epoch 434/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2655 - acc: 0.9055 - val_loss: 0.2590 - val_acc: 0.9100\n",
      "Epoch 435/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.2681 - acc: 0.9060 - val_loss: 0.2580 - val_acc: 0.9120\n",
      "Epoch 436/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2689 - acc: 0.9040 - val_loss: 0.2582 - val_acc: 0.9090\n",
      "Epoch 437/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.2675 - acc: 0.9040 - val_loss: 0.2585 - val_acc: 0.9090\n",
      "Epoch 438/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2604 - acc: 0.9085 - val_loss: 0.2585 - val_acc: 0.9080\n",
      "Epoch 439/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2609 - acc: 0.9020 - val_loss: 0.2580 - val_acc: 0.9100\n",
      "Epoch 440/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2671 - acc: 0.9030 - val_loss: 0.2571 - val_acc: 0.9100\n",
      "Epoch 441/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2622 - acc: 0.9075 - val_loss: 0.2571 - val_acc: 0.9120\n",
      "Epoch 442/800\n",
      "2000/2000 [==============================] - 0s 70us/step - loss: 0.2619 - acc: 0.9055 - val_loss: 0.2597 - val_acc: 0.9090\n",
      "Epoch 443/800\n",
      "2000/2000 [==============================] - 0s 45us/step - loss: 0.2616 - acc: 0.9080 - val_loss: 0.2573 - val_acc: 0.9110\n",
      "Epoch 444/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2694 - acc: 0.9015 - val_loss: 0.2569 - val_acc: 0.9110\n",
      "Epoch 445/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.2548 - acc: 0.9050 - val_loss: 0.2572 - val_acc: 0.9090\n",
      "Epoch 446/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.2566 - acc: 0.9110 - val_loss: 0.2561 - val_acc: 0.9120\n",
      "Epoch 447/800\n",
      "2000/2000 [==============================] - 0s 45us/step - loss: 0.2641 - acc: 0.9050 - val_loss: 0.2564 - val_acc: 0.9120\n",
      "Epoch 448/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2632 - acc: 0.9015 - val_loss: 0.2562 - val_acc: 0.9110\n",
      "Epoch 449/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.2528 - acc: 0.9110 - val_loss: 0.2564 - val_acc: 0.9090\n",
      "Epoch 450/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.2658 - acc: 0.9045 - val_loss: 0.2565 - val_acc: 0.9120\n",
      "Epoch 451/800\n",
      "2000/2000 [==============================] - 0s 55us/step - loss: 0.2570 - acc: 0.9060 - val_loss: 0.2554 - val_acc: 0.9150\n",
      "Epoch 452/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.2693 - acc: 0.9025 - val_loss: 0.2553 - val_acc: 0.9140\n",
      "Epoch 453/800\n",
      "2000/2000 [==============================] - 0s 46us/step - loss: 0.2536 - acc: 0.9110 - val_loss: 0.2554 - val_acc: 0.9140\n",
      "Epoch 454/800\n",
      "2000/2000 [==============================] - 0s 74us/step - loss: 0.2635 - acc: 0.9020 - val_loss: 0.2566 - val_acc: 0.9120\n",
      "Epoch 455/800\n",
      "2000/2000 [==============================] - 0s 72us/step - loss: 0.2652 - acc: 0.8995 - val_loss: 0.2550 - val_acc: 0.9130\n",
      "Epoch 456/800\n",
      "2000/2000 [==============================] - 0s 48us/step - loss: 0.2658 - acc: 0.9075 - val_loss: 0.2552 - val_acc: 0.9130\n",
      "Epoch 457/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.2625 - acc: 0.9025 - val_loss: 0.2581 - val_acc: 0.9110\n",
      "Epoch 458/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2597 - acc: 0.9060 - val_loss: 0.2548 - val_acc: 0.9140\n",
      "Epoch 459/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2609 - acc: 0.9040 - val_loss: 0.2539 - val_acc: 0.9140\n",
      "Epoch 460/800\n",
      "2000/2000 [==============================] - 0s 44us/step - loss: 0.2573 - acc: 0.9070 - val_loss: 0.2540 - val_acc: 0.9130\n",
      "Epoch 461/800\n",
      "2000/2000 [==============================] - 0s 46us/step - loss: 0.2565 - acc: 0.9045 - val_loss: 0.2559 - val_acc: 0.9110\n",
      "Epoch 462/800\n",
      "2000/2000 [==============================] - 0s 50us/step - loss: 0.2554 - acc: 0.9040 - val_loss: 0.2537 - val_acc: 0.9120\n",
      "Epoch 463/800\n",
      "2000/2000 [==============================] - 0s 56us/step - loss: 0.2605 - acc: 0.9100 - val_loss: 0.2530 - val_acc: 0.9150\n",
      "Epoch 464/800\n",
      "2000/2000 [==============================] - 0s 60us/step - loss: 0.2625 - acc: 0.9065 - val_loss: 0.2537 - val_acc: 0.9140\n",
      "Epoch 465/800\n",
      "2000/2000 [==============================] - 0s 67us/step - loss: 0.2635 - acc: 0.9075 - val_loss: 0.2539 - val_acc: 0.9110\n",
      "Epoch 466/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 57us/step - loss: 0.2537 - acc: 0.9055 - val_loss: 0.2526 - val_acc: 0.9130\n",
      "Epoch 467/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.2573 - acc: 0.9080 - val_loss: 0.2542 - val_acc: 0.9120\n",
      "Epoch 468/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.2633 - acc: 0.9045 - val_loss: 0.2526 - val_acc: 0.9130\n",
      "Epoch 469/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2567 - acc: 0.9115 - val_loss: 0.2519 - val_acc: 0.9150\n",
      "Epoch 470/800\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2502 - acc: 0.9115 - val_loss: 0.2522 - val_acc: 0.9140\n",
      "Epoch 471/800\n",
      "2000/2000 [==============================] - 0s 44us/step - loss: 0.2668 - acc: 0.9100 - val_loss: 0.2537 - val_acc: 0.9120\n",
      "Epoch 472/800\n",
      "2000/2000 [==============================] - 0s 42us/step - loss: 0.2538 - acc: 0.9080 - val_loss: 0.2522 - val_acc: 0.9120\n",
      "Epoch 473/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.2621 - acc: 0.9020 - val_loss: 0.2514 - val_acc: 0.9160\n",
      "Epoch 474/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2553 - acc: 0.9050 - val_loss: 0.2508 - val_acc: 0.9140\n",
      "Epoch 475/800\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2646 - acc: 0.9025 - val_loss: 0.2517 - val_acc: 0.9120\n",
      "Epoch 476/800\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2535 - acc: 0.9070 - val_loss: 0.2505 - val_acc: 0.9140\n",
      "Epoch 477/800\n",
      "2000/2000 [==============================] - 0s 59us/step - loss: 0.2528 - acc: 0.9080 - val_loss: 0.2504 - val_acc: 0.9170\n",
      "Epoch 478/800\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 0.2611 - acc: 0.9100 - val_loss: 0.2521 - val_acc: 0.9110\n",
      "Epoch 479/800\n",
      "2000/2000 [==============================] - 0s 57us/step - loss: 0.2558 - acc: 0.9095 - val_loss: 0.2508 - val_acc: 0.9160\n",
      "Epoch 480/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.2580 - acc: 0.9110 - val_loss: 0.2497 - val_acc: 0.9160\n",
      "Epoch 481/800\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2546 - acc: 0.9135 - val_loss: 0.2496 - val_acc: 0.9160\n",
      "Epoch 482/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.2595 - acc: 0.9065 - val_loss: 0.2495 - val_acc: 0.9150\n",
      "Epoch 483/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.2579 - acc: 0.9055 - val_loss: 0.2497 - val_acc: 0.9160\n",
      "Epoch 484/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2542 - acc: 0.9080 - val_loss: 0.2502 - val_acc: 0.9150\n",
      "Epoch 485/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.2537 - acc: 0.9130 - val_loss: 0.2492 - val_acc: 0.9160\n",
      "Epoch 486/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.2582 - acc: 0.9025 - val_loss: 0.2494 - val_acc: 0.9150\n",
      "Epoch 487/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.2584 - acc: 0.9080 - val_loss: 0.2506 - val_acc: 0.9140\n",
      "Epoch 488/800\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 0.2547 - acc: 0.9035 - val_loss: 0.2521 - val_acc: 0.9120\n",
      "Epoch 489/800\n",
      "2000/2000 [==============================] - 0s 40us/step - loss: 0.2528 - acc: 0.9085 - val_loss: 0.2502 - val_acc: 0.9140\n",
      "Epoch 490/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2559 - acc: 0.9095 - val_loss: 0.2509 - val_acc: 0.9130\n",
      "Epoch 491/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.2499 - acc: 0.9045 - val_loss: 0.2481 - val_acc: 0.9140\n",
      "Epoch 492/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2585 - acc: 0.9050 - val_loss: 0.2478 - val_acc: 0.9140\n",
      "Epoch 493/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.2554 - acc: 0.9070 - val_loss: 0.2475 - val_acc: 0.9160\n",
      "Epoch 494/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2507 - acc: 0.9035 - val_loss: 0.2479 - val_acc: 0.9160\n",
      "Epoch 495/800\n",
      "2000/2000 [==============================] - 0s 44us/step - loss: 0.2500 - acc: 0.9055 - val_loss: 0.2475 - val_acc: 0.9150\n",
      "Epoch 496/800\n",
      "2000/2000 [==============================] - 0s 42us/step - loss: 0.2511 - acc: 0.9110 - val_loss: 0.2490 - val_acc: 0.9130\n",
      "Epoch 497/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2571 - acc: 0.9075 - val_loss: 0.2475 - val_acc: 0.9160\n",
      "Epoch 498/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2525 - acc: 0.9070 - val_loss: 0.2482 - val_acc: 0.9150\n",
      "Epoch 499/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2556 - acc: 0.9085 - val_loss: 0.2469 - val_acc: 0.9150\n",
      "Epoch 500/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.2559 - acc: 0.9065 - val_loss: 0.2472 - val_acc: 0.9150\n",
      "Epoch 501/800\n",
      "2000/2000 [==============================] - 0s 46us/step - loss: 0.2585 - acc: 0.9050 - val_loss: 0.2465 - val_acc: 0.9160\n",
      "Epoch 502/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.2548 - acc: 0.9075 - val_loss: 0.2468 - val_acc: 0.9160\n",
      "Epoch 503/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2505 - acc: 0.9025 - val_loss: 0.2463 - val_acc: 0.9180\n",
      "Epoch 504/800\n",
      "2000/2000 [==============================] - 0s 47us/step - loss: 0.2509 - acc: 0.9070 - val_loss: 0.2468 - val_acc: 0.9180\n",
      "Epoch 505/800\n",
      "2000/2000 [==============================] - 0s 47us/step - loss: 0.2512 - acc: 0.9125 - val_loss: 0.2472 - val_acc: 0.9120\n",
      "Epoch 506/800\n",
      "2000/2000 [==============================] - 0s 54us/step - loss: 0.2490 - acc: 0.9125 - val_loss: 0.2457 - val_acc: 0.9170\n",
      "Epoch 507/800\n",
      "2000/2000 [==============================] - 0s 46us/step - loss: 0.2570 - acc: 0.9100 - val_loss: 0.2470 - val_acc: 0.9180\n",
      "Epoch 508/800\n",
      "2000/2000 [==============================] - 0s 44us/step - loss: 0.2496 - acc: 0.9115 - val_loss: 0.2469 - val_acc: 0.9140\n",
      "Epoch 509/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2465 - acc: 0.9085 - val_loss: 0.2454 - val_acc: 0.9170\n",
      "Epoch 510/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.2461 - acc: 0.9055 - val_loss: 0.2454 - val_acc: 0.9160\n",
      "Epoch 511/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2484 - acc: 0.9085 - val_loss: 0.2454 - val_acc: 0.9190\n",
      "Epoch 512/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.2493 - acc: 0.9090 - val_loss: 0.2445 - val_acc: 0.9180\n",
      "Epoch 513/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.2554 - acc: 0.9080 - val_loss: 0.2459 - val_acc: 0.9140\n",
      "Epoch 514/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.2476 - acc: 0.9105 - val_loss: 0.2442 - val_acc: 0.9180\n",
      "Epoch 515/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2556 - acc: 0.9030 - val_loss: 0.2436 - val_acc: 0.9190\n",
      "Epoch 516/800\n",
      "2000/2000 [==============================] - 0s 45us/step - loss: 0.2521 - acc: 0.9095 - val_loss: 0.2444 - val_acc: 0.9180\n",
      "Epoch 517/800\n",
      "2000/2000 [==============================] - 0s 45us/step - loss: 0.2525 - acc: 0.9095 - val_loss: 0.2445 - val_acc: 0.9180\n",
      "Epoch 518/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2464 - acc: 0.9115 - val_loss: 0.2449 - val_acc: 0.9160\n",
      "Epoch 519/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.2520 - acc: 0.9080 - val_loss: 0.2461 - val_acc: 0.9130\n",
      "Epoch 520/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2596 - acc: 0.9075 - val_loss: 0.2445 - val_acc: 0.9180\n",
      "Epoch 521/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2477 - acc: 0.9065 - val_loss: 0.2438 - val_acc: 0.9170\n",
      "Epoch 522/800\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2398 - acc: 0.9135 - val_loss: 0.2433 - val_acc: 0.9170\n",
      "Epoch 523/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.2547 - acc: 0.9075 - val_loss: 0.2449 - val_acc: 0.9170\n",
      "Epoch 524/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2546 - acc: 0.9115 - val_loss: 0.2436 - val_acc: 0.9170\n",
      "Epoch 525/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2561 - acc: 0.9055 - val_loss: 0.2442 - val_acc: 0.9170\n",
      "Epoch 526/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.2483 - acc: 0.9080 - val_loss: 0.2431 - val_acc: 0.9180\n",
      "Epoch 527/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2593 - acc: 0.9065 - val_loss: 0.2426 - val_acc: 0.9190\n",
      "Epoch 528/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2573 - acc: 0.9030 - val_loss: 0.2433 - val_acc: 0.9170\n",
      "Epoch 529/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2508 - acc: 0.9075 - val_loss: 0.2425 - val_acc: 0.9190\n",
      "Epoch 530/800\n",
      "2000/2000 [==============================] - 0s 47us/step - loss: 0.2595 - acc: 0.9035 - val_loss: 0.2426 - val_acc: 0.9190\n",
      "Epoch 531/800\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.2445 - acc: 0.9135 - val_loss: 0.2434 - val_acc: 0.9170\n",
      "Epoch 532/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.2516 - acc: 0.9100 - val_loss: 0.2418 - val_acc: 0.9190\n",
      "Epoch 533/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2355 - acc: 0.9155 - val_loss: 0.2416 - val_acc: 0.9190\n",
      "Epoch 534/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2536 - acc: 0.9060 - val_loss: 0.2432 - val_acc: 0.9190\n",
      "Epoch 535/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2527 - acc: 0.9115 - val_loss: 0.2412 - val_acc: 0.9180\n",
      "Epoch 536/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2479 - acc: 0.9105 - val_loss: 0.2428 - val_acc: 0.9160\n",
      "Epoch 537/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2422 - acc: 0.9110 - val_loss: 0.2419 - val_acc: 0.9180\n",
      "Epoch 538/800\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.2424 - acc: 0.9105 - val_loss: 0.2432 - val_acc: 0.9140\n",
      "Epoch 539/800\n",
      "2000/2000 [==============================] - 0s 59us/step - loss: 0.2480 - acc: 0.9125 - val_loss: 0.2420 - val_acc: 0.9190\n",
      "Epoch 540/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.2455 - acc: 0.9085 - val_loss: 0.2415 - val_acc: 0.9180\n",
      "Epoch 541/800\n",
      "2000/2000 [==============================] - 0s 50us/step - loss: 0.2393 - acc: 0.9160 - val_loss: 0.2421 - val_acc: 0.9180\n",
      "Epoch 542/800\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.2428 - acc: 0.9145 - val_loss: 0.2418 - val_acc: 0.9180\n",
      "Epoch 543/800\n",
      "2000/2000 [==============================] - 0s 69us/step - loss: 0.2477 - acc: 0.9115 - val_loss: 0.2417 - val_acc: 0.9180\n",
      "Epoch 544/800\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 0.2538 - acc: 0.9065 - val_loss: 0.2402 - val_acc: 0.9200\n",
      "Epoch 545/800\n",
      "2000/2000 [==============================] - 0s 73us/step - loss: 0.2522 - acc: 0.9090 - val_loss: 0.2406 - val_acc: 0.9200\n",
      "Epoch 546/800\n",
      "2000/2000 [==============================] - 0s 42us/step - loss: 0.2510 - acc: 0.9045 - val_loss: 0.2401 - val_acc: 0.9210\n",
      "Epoch 547/800\n",
      "2000/2000 [==============================] - 0s 47us/step - loss: 0.2505 - acc: 0.9100 - val_loss: 0.2403 - val_acc: 0.9210\n",
      "Epoch 548/800\n",
      "2000/2000 [==============================] - 0s 53us/step - loss: 0.2527 - acc: 0.9100 - val_loss: 0.2406 - val_acc: 0.9190\n",
      "Epoch 549/800\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 0.2400 - acc: 0.9110 - val_loss: 0.2397 - val_acc: 0.9200\n",
      "Epoch 550/800\n",
      "2000/2000 [==============================] - 0s 78us/step - loss: 0.2438 - acc: 0.9100 - val_loss: 0.2395 - val_acc: 0.9220\n",
      "Epoch 551/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.2551 - acc: 0.9070 - val_loss: 0.2389 - val_acc: 0.9210\n",
      "Epoch 552/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2477 - acc: 0.9105 - val_loss: 0.2399 - val_acc: 0.9230\n",
      "Epoch 553/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2530 - acc: 0.9025 - val_loss: 0.2421 - val_acc: 0.9170\n",
      "Epoch 554/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2446 - acc: 0.9075 - val_loss: 0.2409 - val_acc: 0.9200\n",
      "Epoch 555/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.2537 - acc: 0.9060 - val_loss: 0.2397 - val_acc: 0.9190\n",
      "Epoch 556/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.2494 - acc: 0.9085 - val_loss: 0.2397 - val_acc: 0.9180\n",
      "Epoch 557/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2518 - acc: 0.9050 - val_loss: 0.2386 - val_acc: 0.9210\n",
      "Epoch 558/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.2552 - acc: 0.9040 - val_loss: 0.2386 - val_acc: 0.9210\n",
      "Epoch 559/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2418 - acc: 0.9125 - val_loss: 0.2378 - val_acc: 0.9230\n",
      "Epoch 560/800\n",
      "2000/2000 [==============================] - 0s 39us/step - loss: 0.2373 - acc: 0.9130 - val_loss: 0.2371 - val_acc: 0.9210\n",
      "Epoch 561/800\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.2424 - acc: 0.9180 - val_loss: 0.2388 - val_acc: 0.9220\n",
      "Epoch 562/800\n",
      "2000/2000 [==============================] - 0s 156us/step - loss: 0.2483 - acc: 0.9080 - val_loss: 0.2375 - val_acc: 0.9230\n",
      "Epoch 563/800\n",
      "2000/2000 [==============================] - 0s 73us/step - loss: 0.2508 - acc: 0.9080 - val_loss: 0.2386 - val_acc: 0.9220\n",
      "Epoch 564/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.2467 - acc: 0.9045 - val_loss: 0.2385 - val_acc: 0.9210\n",
      "Epoch 565/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.2468 - acc: 0.9115 - val_loss: 0.2377 - val_acc: 0.9210\n",
      "Epoch 566/800\n",
      "2000/2000 [==============================] - 0s 48us/step - loss: 0.2440 - acc: 0.9105 - val_loss: 0.2375 - val_acc: 0.9220\n",
      "Epoch 567/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.2428 - acc: 0.9100 - val_loss: 0.2377 - val_acc: 0.9220\n",
      "Epoch 568/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.2473 - acc: 0.9120 - val_loss: 0.2378 - val_acc: 0.9200\n",
      "Epoch 569/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2471 - acc: 0.9115 - val_loss: 0.2366 - val_acc: 0.9220\n",
      "Epoch 570/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.2487 - acc: 0.9100 - val_loss: 0.2376 - val_acc: 0.9220\n",
      "Epoch 571/800\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.2446 - acc: 0.9095 - val_loss: 0.2371 - val_acc: 0.9240\n",
      "Epoch 572/800\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.2400 - acc: 0.9115 - val_loss: 0.2384 - val_acc: 0.9210\n",
      "Epoch 573/800\n",
      "2000/2000 [==============================] - 0s 64us/step - loss: 0.2461 - acc: 0.9110 - val_loss: 0.2375 - val_acc: 0.9200\n",
      "Epoch 574/800\n",
      "2000/2000 [==============================] - 0s 44us/step - loss: 0.2370 - acc: 0.9135 - val_loss: 0.2363 - val_acc: 0.9220\n",
      "Epoch 575/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.2410 - acc: 0.9090 - val_loss: 0.2360 - val_acc: 0.9200\n",
      "Epoch 576/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2390 - acc: 0.9135 - val_loss: 0.2364 - val_acc: 0.9220\n",
      "Epoch 577/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2428 - acc: 0.9125 - val_loss: 0.2384 - val_acc: 0.9210\n",
      "Epoch 578/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2431 - acc: 0.9120 - val_loss: 0.2356 - val_acc: 0.9220\n",
      "Epoch 579/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2328 - acc: 0.9175 - val_loss: 0.2354 - val_acc: 0.9220\n",
      "Epoch 580/800\n",
      "2000/2000 [==============================] - 0s 54us/step - loss: 0.2428 - acc: 0.9125 - val_loss: 0.2359 - val_acc: 0.9220\n",
      "Epoch 581/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2498 - acc: 0.9090 - val_loss: 0.2356 - val_acc: 0.9230\n",
      "Epoch 582/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2434 - acc: 0.9080 - val_loss: 0.2355 - val_acc: 0.9240\n",
      "Epoch 583/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2407 - acc: 0.9120 - val_loss: 0.2358 - val_acc: 0.9240\n",
      "Epoch 584/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2402 - acc: 0.9130 - val_loss: 0.2361 - val_acc: 0.9230\n",
      "Epoch 585/800\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2433 - acc: 0.9115 - val_loss: 0.2365 - val_acc: 0.9210\n",
      "Epoch 586/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2435 - acc: 0.9110 - val_loss: 0.2356 - val_acc: 0.9230\n",
      "Epoch 587/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2369 - acc: 0.9180 - val_loss: 0.2390 - val_acc: 0.9200\n",
      "Epoch 588/800\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2446 - acc: 0.9085 - val_loss: 0.2360 - val_acc: 0.9200\n",
      "Epoch 589/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2405 - acc: 0.9080 - val_loss: 0.2349 - val_acc: 0.9240\n",
      "Epoch 590/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2418 - acc: 0.9120 - val_loss: 0.2352 - val_acc: 0.9210\n",
      "Epoch 591/800\n",
      "2000/2000 [==============================] - 0s 62us/step - loss: 0.2474 - acc: 0.9115 - val_loss: 0.2362 - val_acc: 0.9190\n",
      "Epoch 592/800\n",
      "2000/2000 [==============================] - 0s 53us/step - loss: 0.2407 - acc: 0.9095 - val_loss: 0.2359 - val_acc: 0.9210\n",
      "Epoch 593/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2412 - acc: 0.9065 - val_loss: 0.2349 - val_acc: 0.9200\n",
      "Epoch 594/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2294 - acc: 0.9140 - val_loss: 0.2357 - val_acc: 0.9250\n",
      "Epoch 595/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2331 - acc: 0.9135 - val_loss: 0.2357 - val_acc: 0.9210\n",
      "Epoch 596/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2396 - acc: 0.9115 - val_loss: 0.2347 - val_acc: 0.9200\n",
      "Epoch 597/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2442 - acc: 0.9120 - val_loss: 0.2336 - val_acc: 0.9240\n",
      "Epoch 598/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2435 - acc: 0.9095 - val_loss: 0.2336 - val_acc: 0.9220\n",
      "Epoch 599/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2436 - acc: 0.9135 - val_loss: 0.2330 - val_acc: 0.9210\n",
      "Epoch 600/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2475 - acc: 0.9100 - val_loss: 0.2341 - val_acc: 0.9220\n",
      "Epoch 601/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2385 - acc: 0.9175 - val_loss: 0.2335 - val_acc: 0.9190\n",
      "Epoch 602/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2391 - acc: 0.9145 - val_loss: 0.2329 - val_acc: 0.9200\n",
      "Epoch 603/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2382 - acc: 0.9100 - val_loss: 0.2337 - val_acc: 0.9210\n",
      "Epoch 604/800\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2444 - acc: 0.9080 - val_loss: 0.2341 - val_acc: 0.9210\n",
      "Epoch 605/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2321 - acc: 0.9155 - val_loss: 0.2345 - val_acc: 0.9250\n",
      "Epoch 606/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2409 - acc: 0.9120 - val_loss: 0.2350 - val_acc: 0.9200\n",
      "Epoch 607/800\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2455 - acc: 0.9090 - val_loss: 0.2337 - val_acc: 0.9240\n",
      "Epoch 608/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.2359 - acc: 0.9105 - val_loss: 0.2351 - val_acc: 0.9200\n",
      "Epoch 609/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2496 - acc: 0.9075 - val_loss: 0.2332 - val_acc: 0.9210\n",
      "Epoch 610/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2430 - acc: 0.9105 - val_loss: 0.2338 - val_acc: 0.9230\n",
      "Epoch 611/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2377 - acc: 0.9120 - val_loss: 0.2334 - val_acc: 0.9220\n",
      "Epoch 612/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2342 - acc: 0.9155 - val_loss: 0.2331 - val_acc: 0.9220\n",
      "Epoch 613/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2402 - acc: 0.9100 - val_loss: 0.2344 - val_acc: 0.9230\n",
      "Epoch 614/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2388 - acc: 0.9130 - val_loss: 0.2334 - val_acc: 0.9220\n",
      "Epoch 615/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2374 - acc: 0.9120 - val_loss: 0.2333 - val_acc: 0.9220\n",
      "Epoch 616/800\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2408 - acc: 0.9110 - val_loss: 0.2321 - val_acc: 0.9240\n",
      "Epoch 617/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2433 - acc: 0.9070 - val_loss: 0.2336 - val_acc: 0.9270\n",
      "Epoch 618/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2489 - acc: 0.9050 - val_loss: 0.2324 - val_acc: 0.9260\n",
      "Epoch 619/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2333 - acc: 0.9150 - val_loss: 0.2332 - val_acc: 0.9200\n",
      "Epoch 620/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2363 - acc: 0.9145 - val_loss: 0.2322 - val_acc: 0.9230\n",
      "Epoch 621/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2436 - acc: 0.9080 - val_loss: 0.2320 - val_acc: 0.9210\n",
      "Epoch 622/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2368 - acc: 0.9105 - val_loss: 0.2357 - val_acc: 0.9200\n",
      "Epoch 623/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2431 - acc: 0.9125 - val_loss: 0.2321 - val_acc: 0.9230\n",
      "Epoch 624/800\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2417 - acc: 0.9090 - val_loss: 0.2334 - val_acc: 0.9220\n",
      "Epoch 625/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2377 - acc: 0.9135 - val_loss: 0.2329 - val_acc: 0.9220\n",
      "Epoch 626/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2388 - acc: 0.9125 - val_loss: 0.2324 - val_acc: 0.9220\n",
      "Epoch 627/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2360 - acc: 0.9185 - val_loss: 0.2326 - val_acc: 0.9230\n",
      "Epoch 628/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2367 - acc: 0.9115 - val_loss: 0.2331 - val_acc: 0.9220\n",
      "Epoch 629/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2335 - acc: 0.9160 - val_loss: 0.2329 - val_acc: 0.9210\n",
      "Epoch 630/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2388 - acc: 0.9135 - val_loss: 0.2323 - val_acc: 0.9270\n",
      "Epoch 631/800\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2442 - acc: 0.9125 - val_loss: 0.2322 - val_acc: 0.9240\n",
      "Epoch 632/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2356 - acc: 0.9125 - val_loss: 0.2315 - val_acc: 0.9230\n",
      "Epoch 633/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2436 - acc: 0.9095 - val_loss: 0.2315 - val_acc: 0.9290\n",
      "Epoch 634/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2458 - acc: 0.9105 - val_loss: 0.2316 - val_acc: 0.9260\n",
      "Epoch 635/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2362 - acc: 0.9150 - val_loss: 0.2311 - val_acc: 0.9250\n",
      "Epoch 636/800\n",
      "2000/2000 [==============================] - 0s 46us/step - loss: 0.2373 - acc: 0.9125 - val_loss: 0.2319 - val_acc: 0.9230\n",
      "Epoch 637/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2379 - acc: 0.9155 - val_loss: 0.2319 - val_acc: 0.9220\n",
      "Epoch 638/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2361 - acc: 0.9125 - val_loss: 0.2313 - val_acc: 0.9230\n",
      "Epoch 639/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2476 - acc: 0.9130 - val_loss: 0.2328 - val_acc: 0.9280\n",
      "Epoch 640/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2394 - acc: 0.9115 - val_loss: 0.2318 - val_acc: 0.9210\n",
      "Epoch 641/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2405 - acc: 0.9110 - val_loss: 0.2309 - val_acc: 0.9210\n",
      "Epoch 642/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2405 - acc: 0.9135 - val_loss: 0.2326 - val_acc: 0.9280\n",
      "Epoch 643/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2334 - acc: 0.9160 - val_loss: 0.2312 - val_acc: 0.9260\n",
      "Epoch 644/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2330 - acc: 0.9155 - val_loss: 0.2316 - val_acc: 0.9250\n",
      "Epoch 645/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2363 - acc: 0.9140 - val_loss: 0.2311 - val_acc: 0.9260\n",
      "Epoch 646/800\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2385 - acc: 0.9080 - val_loss: 0.2302 - val_acc: 0.9250\n",
      "Epoch 647/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2347 - acc: 0.9130 - val_loss: 0.2305 - val_acc: 0.9250\n",
      "Epoch 648/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2405 - acc: 0.9155 - val_loss: 0.2307 - val_acc: 0.9290\n",
      "Epoch 649/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2324 - acc: 0.9150 - val_loss: 0.2310 - val_acc: 0.9200\n",
      "Epoch 650/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2387 - acc: 0.9115 - val_loss: 0.2312 - val_acc: 0.9250\n",
      "Epoch 651/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2318 - acc: 0.9175 - val_loss: 0.2308 - val_acc: 0.9250\n",
      "Epoch 652/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2310 - acc: 0.9160 - val_loss: 0.2312 - val_acc: 0.9240\n",
      "Epoch 653/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2347 - acc: 0.9135 - val_loss: 0.2325 - val_acc: 0.9280\n",
      "Epoch 654/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2380 - acc: 0.9145 - val_loss: 0.2307 - val_acc: 0.9280\n",
      "Epoch 655/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2376 - acc: 0.9110 - val_loss: 0.2305 - val_acc: 0.9260\n",
      "Epoch 656/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.2424 - acc: 0.9130 - val_loss: 0.2327 - val_acc: 0.9250\n",
      "Epoch 657/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2362 - acc: 0.9150 - val_loss: 0.2302 - val_acc: 0.9220\n",
      "Epoch 658/800\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2453 - acc: 0.9090 - val_loss: 0.2301 - val_acc: 0.9250\n",
      "Epoch 659/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2409 - acc: 0.9140 - val_loss: 0.2306 - val_acc: 0.9250\n",
      "Epoch 660/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2306 - acc: 0.9155 - val_loss: 0.2298 - val_acc: 0.9280\n",
      "Epoch 661/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2286 - acc: 0.9170 - val_loss: 0.2299 - val_acc: 0.9280\n",
      "Epoch 662/800\n",
      "2000/2000 [==============================] - 0s 17us/step - loss: 0.2373 - acc: 0.9075 - val_loss: 0.2290 - val_acc: 0.9250\n",
      "Epoch 663/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2363 - acc: 0.9135 - val_loss: 0.2288 - val_acc: 0.9270\n",
      "Epoch 664/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2432 - acc: 0.9110 - val_loss: 0.2281 - val_acc: 0.9270\n",
      "Epoch 665/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2405 - acc: 0.9110 - val_loss: 0.2285 - val_acc: 0.9280\n",
      "Epoch 666/800\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2390 - acc: 0.9105 - val_loss: 0.2278 - val_acc: 0.9280\n",
      "Epoch 667/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2328 - acc: 0.9160 - val_loss: 0.2278 - val_acc: 0.9300\n",
      "Epoch 668/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2321 - acc: 0.9175 - val_loss: 0.2294 - val_acc: 0.9240\n",
      "Epoch 669/800\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2453 - acc: 0.9100 - val_loss: 0.2285 - val_acc: 0.9300\n",
      "Epoch 670/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2348 - acc: 0.9140 - val_loss: 0.2296 - val_acc: 0.9300\n",
      "Epoch 671/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2394 - acc: 0.9060 - val_loss: 0.2292 - val_acc: 0.9250\n",
      "Epoch 672/800\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2442 - acc: 0.9080 - val_loss: 0.2296 - val_acc: 0.9270\n",
      "Epoch 673/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2311 - acc: 0.9165 - val_loss: 0.2293 - val_acc: 0.9280\n",
      "Epoch 674/800\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2355 - acc: 0.9120 - val_loss: 0.2293 - val_acc: 0.9280\n",
      "Epoch 675/800\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2420 - acc: 0.9115 - val_loss: 0.2299 - val_acc: 0.9260\n",
      "Epoch 676/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2360 - acc: 0.9130 - val_loss: 0.2287 - val_acc: 0.9270\n",
      "Epoch 677/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2365 - acc: 0.9125 - val_loss: 0.2283 - val_acc: 0.9270\n",
      "Epoch 678/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2380 - acc: 0.9125 - val_loss: 0.2282 - val_acc: 0.9280\n",
      "Epoch 679/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2370 - acc: 0.9125 - val_loss: 0.2290 - val_acc: 0.9280\n",
      "Epoch 680/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2357 - acc: 0.9155 - val_loss: 0.2321 - val_acc: 0.9220\n",
      "Epoch 681/800\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2408 - acc: 0.9105 - val_loss: 0.2279 - val_acc: 0.9270\n",
      "Epoch 682/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2344 - acc: 0.9165 - val_loss: 0.2286 - val_acc: 0.9280\n",
      "Epoch 683/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2404 - acc: 0.9095 - val_loss: 0.2278 - val_acc: 0.9280\n",
      "Epoch 684/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2259 - acc: 0.9220 - val_loss: 0.2280 - val_acc: 0.9290\n",
      "Epoch 685/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2364 - acc: 0.9130 - val_loss: 0.2277 - val_acc: 0.9270\n",
      "Epoch 686/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2351 - acc: 0.9120 - val_loss: 0.2275 - val_acc: 0.9300\n",
      "Epoch 687/800\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2277 - acc: 0.9175 - val_loss: 0.2276 - val_acc: 0.9290\n",
      "Epoch 688/800\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2364 - acc: 0.9135 - val_loss: 0.2272 - val_acc: 0.9280\n",
      "Epoch 689/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2395 - acc: 0.9105 - val_loss: 0.2274 - val_acc: 0.9300\n",
      "Epoch 690/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2353 - acc: 0.9140 - val_loss: 0.2269 - val_acc: 0.9290\n",
      "Epoch 691/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2412 - acc: 0.9135 - val_loss: 0.2273 - val_acc: 0.9260\n",
      "Epoch 692/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2334 - acc: 0.9120 - val_loss: 0.2273 - val_acc: 0.9280\n",
      "Epoch 693/800\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2322 - acc: 0.9110 - val_loss: 0.2272 - val_acc: 0.9290\n",
      "Epoch 694/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2287 - acc: 0.9175 - val_loss: 0.2268 - val_acc: 0.9290\n",
      "Epoch 695/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2282 - acc: 0.9190 - val_loss: 0.2273 - val_acc: 0.9300\n",
      "Epoch 696/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2328 - acc: 0.9135 - val_loss: 0.2274 - val_acc: 0.9300\n",
      "Epoch 697/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2423 - acc: 0.9100 - val_loss: 0.2270 - val_acc: 0.9270\n",
      "Epoch 698/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2324 - acc: 0.9170 - val_loss: 0.2263 - val_acc: 0.9290\n",
      "Epoch 699/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2328 - acc: 0.9150 - val_loss: 0.2264 - val_acc: 0.9270\n",
      "Epoch 700/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2332 - acc: 0.9170 - val_loss: 0.2268 - val_acc: 0.9290\n",
      "Epoch 701/800\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.2314 - acc: 0.920 - 0s 36us/step - loss: 0.2362 - acc: 0.9155 - val_loss: 0.2264 - val_acc: 0.9280\n",
      "Epoch 702/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2396 - acc: 0.9090 - val_loss: 0.2268 - val_acc: 0.9280\n",
      "Epoch 703/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2326 - acc: 0.9205 - val_loss: 0.2277 - val_acc: 0.9290\n",
      "Epoch 704/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2277 - acc: 0.9140 - val_loss: 0.2264 - val_acc: 0.9300\n",
      "Epoch 705/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2347 - acc: 0.9125 - val_loss: 0.2262 - val_acc: 0.9310\n",
      "Epoch 706/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.2314 - acc: 0.9160 - val_loss: 0.2256 - val_acc: 0.9290\n",
      "Epoch 707/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2296 - acc: 0.9165 - val_loss: 0.2265 - val_acc: 0.9290\n",
      "Epoch 708/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2352 - acc: 0.9155 - val_loss: 0.2257 - val_acc: 0.9290\n",
      "Epoch 709/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2267 - acc: 0.9210 - val_loss: 0.2271 - val_acc: 0.9300\n",
      "Epoch 710/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2356 - acc: 0.9135 - val_loss: 0.2265 - val_acc: 0.9280\n",
      "Epoch 711/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2245 - acc: 0.9155 - val_loss: 0.2256 - val_acc: 0.9300\n",
      "Epoch 712/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2340 - acc: 0.9155 - val_loss: 0.2254 - val_acc: 0.9290\n",
      "Epoch 713/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2280 - acc: 0.9175 - val_loss: 0.2254 - val_acc: 0.9290\n",
      "Epoch 714/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2327 - acc: 0.9130 - val_loss: 0.2252 - val_acc: 0.9290\n",
      "Epoch 715/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2309 - acc: 0.9175 - val_loss: 0.2262 - val_acc: 0.9310\n",
      "Epoch 716/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2254 - acc: 0.9195 - val_loss: 0.2263 - val_acc: 0.9290\n",
      "Epoch 717/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2387 - acc: 0.9145 - val_loss: 0.2261 - val_acc: 0.9290\n",
      "Epoch 718/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.2244 - acc: 0.9190 - val_loss: 0.2265 - val_acc: 0.9290\n",
      "Epoch 719/800\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.2233 - acc: 0.9175 - val_loss: 0.2269 - val_acc: 0.9280\n",
      "Epoch 720/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.2368 - acc: 0.9105 - val_loss: 0.2268 - val_acc: 0.9270\n",
      "Epoch 721/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2291 - acc: 0.9165 - val_loss: 0.2256 - val_acc: 0.9280\n",
      "Epoch 722/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2295 - acc: 0.9175 - val_loss: 0.2259 - val_acc: 0.9290\n",
      "Epoch 723/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2280 - acc: 0.9195 - val_loss: 0.2253 - val_acc: 0.9290\n",
      "Epoch 724/800\n",
      "2000/2000 [==============================] - 0s 17us/step - loss: 0.2317 - acc: 0.9185 - val_loss: 0.2258 - val_acc: 0.9300\n",
      "Epoch 725/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2394 - acc: 0.9080 - val_loss: 0.2264 - val_acc: 0.9290\n",
      "Epoch 726/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2384 - acc: 0.9155 - val_loss: 0.2255 - val_acc: 0.9290\n",
      "Epoch 727/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2280 - acc: 0.9135 - val_loss: 0.2255 - val_acc: 0.9300\n",
      "Epoch 728/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2342 - acc: 0.9130 - val_loss: 0.2264 - val_acc: 0.9280\n",
      "Epoch 729/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2279 - acc: 0.9145 - val_loss: 0.2262 - val_acc: 0.9280\n",
      "Epoch 730/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2308 - acc: 0.9140 - val_loss: 0.2254 - val_acc: 0.9300\n",
      "Epoch 731/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2373 - acc: 0.9155 - val_loss: 0.2254 - val_acc: 0.9300\n",
      "Epoch 732/800\n",
      "2000/2000 [==============================] - 0s 44us/step - loss: 0.2323 - acc: 0.9160 - val_loss: 0.2255 - val_acc: 0.9300\n",
      "Epoch 733/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2306 - acc: 0.9155 - val_loss: 0.2248 - val_acc: 0.9280\n",
      "Epoch 734/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2351 - acc: 0.9130 - val_loss: 0.2242 - val_acc: 0.9290\n",
      "Epoch 735/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2248 - acc: 0.9180 - val_loss: 0.2246 - val_acc: 0.9280\n",
      "Epoch 736/800\n",
      "2000/2000 [==============================] - 0s 17us/step - loss: 0.2268 - acc: 0.9160 - val_loss: 0.2247 - val_acc: 0.9280\n",
      "Epoch 737/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2267 - acc: 0.9160 - val_loss: 0.2256 - val_acc: 0.9290\n",
      "Epoch 738/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2334 - acc: 0.9135 - val_loss: 0.2237 - val_acc: 0.9280\n",
      "Epoch 739/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2413 - acc: 0.9110 - val_loss: 0.2246 - val_acc: 0.9280\n",
      "Epoch 740/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2323 - acc: 0.9145 - val_loss: 0.2252 - val_acc: 0.9300\n",
      "Epoch 741/800\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2292 - acc: 0.9165 - val_loss: 0.2257 - val_acc: 0.9270\n",
      "Epoch 742/800\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.2397 - acc: 0.9070 - val_loss: 0.2241 - val_acc: 0.9300\n",
      "Epoch 743/800\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.2343 - acc: 0.9095 - val_loss: 0.2238 - val_acc: 0.9280\n",
      "Epoch 744/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2320 - acc: 0.9140 - val_loss: 0.2237 - val_acc: 0.9280\n",
      "Epoch 745/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2303 - acc: 0.9100 - val_loss: 0.2235 - val_acc: 0.9280\n",
      "Epoch 746/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2368 - acc: 0.9180 - val_loss: 0.2254 - val_acc: 0.9260\n",
      "Epoch 747/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2265 - acc: 0.9165 - val_loss: 0.2246 - val_acc: 0.9280\n",
      "Epoch 748/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2403 - acc: 0.9170 - val_loss: 0.2244 - val_acc: 0.9290\n",
      "Epoch 749/800\n",
      "2000/2000 [==============================] - 0s 17us/step - loss: 0.2261 - acc: 0.9170 - val_loss: 0.2241 - val_acc: 0.9300\n",
      "Epoch 750/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2274 - acc: 0.9160 - val_loss: 0.2256 - val_acc: 0.9270\n",
      "Epoch 751/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2351 - acc: 0.9160 - val_loss: 0.2260 - val_acc: 0.9250\n",
      "Epoch 752/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2270 - acc: 0.9160 - val_loss: 0.2250 - val_acc: 0.9280\n",
      "Epoch 753/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2358 - acc: 0.9155 - val_loss: 0.2244 - val_acc: 0.9280\n",
      "Epoch 754/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2295 - acc: 0.9175 - val_loss: 0.2240 - val_acc: 0.9280\n",
      "Epoch 755/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2221 - acc: 0.9180 - val_loss: 0.2242 - val_acc: 0.9280\n",
      "Epoch 756/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2331 - acc: 0.9180 - val_loss: 0.2236 - val_acc: 0.9280\n",
      "Epoch 757/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2328 - acc: 0.9190 - val_loss: 0.2243 - val_acc: 0.9280\n",
      "Epoch 758/800\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.2286 - acc: 0.9180 - val_loss: 0.2246 - val_acc: 0.9260\n",
      "Epoch 759/800\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2312 - acc: 0.9175 - val_loss: 0.2246 - val_acc: 0.9270\n",
      "Epoch 760/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2241 - acc: 0.9210 - val_loss: 0.2240 - val_acc: 0.9260\n",
      "Epoch 761/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2347 - acc: 0.9150 - val_loss: 0.2235 - val_acc: 0.9290\n",
      "Epoch 762/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2395 - acc: 0.9090 - val_loss: 0.2237 - val_acc: 0.9280\n",
      "Epoch 763/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2291 - acc: 0.9195 - val_loss: 0.2239 - val_acc: 0.9270\n",
      "Epoch 764/800\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2253 - acc: 0.9190 - val_loss: 0.2232 - val_acc: 0.9280\n",
      "Epoch 765/800\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.2259 - acc: 0.9205 - val_loss: 0.2248 - val_acc: 0.9260\n",
      "Epoch 766/800\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2311 - acc: 0.9155 - val_loss: 0.2233 - val_acc: 0.9270\n",
      "Epoch 767/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2216 - acc: 0.9195 - val_loss: 0.2237 - val_acc: 0.9280\n",
      "Epoch 768/800\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.2209 - acc: 0.9190 - val_loss: 0.2236 - val_acc: 0.9270\n",
      "Epoch 769/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2272 - acc: 0.9145 - val_loss: 0.2247 - val_acc: 0.9280\n",
      "Epoch 770/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2290 - acc: 0.9205 - val_loss: 0.2241 - val_acc: 0.9270\n",
      "Epoch 771/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2330 - acc: 0.9140 - val_loss: 0.2233 - val_acc: 0.9270\n",
      "Epoch 772/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2281 - acc: 0.9155 - val_loss: 0.2242 - val_acc: 0.9270\n",
      "Epoch 773/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2223 - acc: 0.9205 - val_loss: 0.2248 - val_acc: 0.9270\n",
      "Epoch 774/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2336 - acc: 0.9160 - val_loss: 0.2243 - val_acc: 0.9260\n",
      "Epoch 775/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2218 - acc: 0.9240 - val_loss: 0.2231 - val_acc: 0.9280\n",
      "Epoch 776/800\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.2203 - acc: 0.9195 - val_loss: 0.2231 - val_acc: 0.9270\n",
      "Epoch 777/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2281 - acc: 0.9185 - val_loss: 0.2237 - val_acc: 0.9270\n",
      "Epoch 778/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2327 - acc: 0.9170 - val_loss: 0.2233 - val_acc: 0.9270\n",
      "Epoch 779/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2229 - acc: 0.9225 - val_loss: 0.2240 - val_acc: 0.9270\n",
      "Epoch 780/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2291 - acc: 0.9170 - val_loss: 0.2234 - val_acc: 0.9260\n",
      "Epoch 781/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2200 - acc: 0.9205 - val_loss: 0.2233 - val_acc: 0.9260\n",
      "Epoch 782/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2282 - acc: 0.9155 - val_loss: 0.2237 - val_acc: 0.9260\n",
      "Epoch 783/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2183 - acc: 0.9200 - val_loss: 0.2265 - val_acc: 0.9240\n",
      "Epoch 784/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2260 - acc: 0.9150 - val_loss: 0.2236 - val_acc: 0.9270\n",
      "Epoch 785/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2242 - acc: 0.9200 - val_loss: 0.2233 - val_acc: 0.9270\n",
      "Epoch 786/800\n",
      "2000/2000 [==============================] - 0s 69us/step - loss: 0.2283 - acc: 0.9195 - val_loss: 0.2235 - val_acc: 0.9260\n",
      "Epoch 787/800\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 0.2226 - acc: 0.9190 - val_loss: 0.2232 - val_acc: 0.9260\n",
      "Epoch 788/800\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.2245 - acc: 0.9180 - val_loss: 0.2230 - val_acc: 0.9270\n",
      "Epoch 789/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2239 - acc: 0.9140 - val_loss: 0.2231 - val_acc: 0.9270\n",
      "Epoch 790/800\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2159 - acc: 0.9215 - val_loss: 0.2224 - val_acc: 0.9270\n",
      "Epoch 791/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2303 - acc: 0.9205 - val_loss: 0.2223 - val_acc: 0.9260\n",
      "Epoch 792/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2276 - acc: 0.9180 - val_loss: 0.2223 - val_acc: 0.9270\n",
      "Epoch 793/800\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2246 - acc: 0.9210 - val_loss: 0.2236 - val_acc: 0.9270\n",
      "Epoch 794/800\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.2288 - acc: 0.9135 - val_loss: 0.2235 - val_acc: 0.9260\n",
      "Epoch 795/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2295 - acc: 0.9115 - val_loss: 0.2223 - val_acc: 0.9260\n",
      "Epoch 796/800\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2319 - acc: 0.9180 - val_loss: 0.2219 - val_acc: 0.9260\n",
      "Epoch 797/800\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.2228 - acc: 0.9205 - val_loss: 0.2222 - val_acc: 0.9260\n",
      "Epoch 798/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2317 - acc: 0.9175 - val_loss: 0.2222 - val_acc: 0.9260\n",
      "Epoch 799/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2314 - acc: 0.9150 - val_loss: 0.2224 - val_acc: 0.9250\n",
      "Epoch 800/800\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2356 - acc: 0.9155 - val_loss: 0.2221 - val_acc: 0.9270\n",
      "Test loss: 0.22205783367156984\n",
      "Test accuracy: 0.927\n"
     ]
    }
   ],
   "source": [
    "# convert class vectors to class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64*2, activation='sigmoid', input_shape=(col-1,)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64*2, activation='sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.9538568e-06 9.6769160e-01 3.2306381e-02]\n",
      " [1.8085571e-03 8.5312420e-01 1.4506727e-01]\n",
      " [8.8720995e-01 5.2870415e-05 1.1273716e-01]\n",
      " ...\n",
      " [3.4390837e-08 9.5405078e-01 4.5949236e-02]\n",
      " [1.7025011e-06 9.2704749e-01 7.2950810e-02]\n",
      " [9.5771277e-01 3.4587254e-05 4.2252637e-02]]\n",
      "(1000, 3)\n"
     ]
    }
   ],
   "source": [
    "classes = model.predict(x_test)\n",
    "print(classes)\n",
    "print(np.shape(classes)) #probability distribution for each of the 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "#plotting the training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "epochs=range(1,len(acc)+1)\n",
    "plt.plot(epochs,loss,'bo',label='Training loss') #bo for blue dot\n",
    "plt.plot(epochs,val_loss,'b',label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "acc=history_dict['acc']\n",
    "val_acc=history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs,acc,'bo',label='Training acc')\n",
    "plt.plot(epochs,val_acc,'b',label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('traj_class_ffnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
